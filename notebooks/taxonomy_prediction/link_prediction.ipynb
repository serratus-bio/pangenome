{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pangenome taxonomy link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook config\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "%load_ext dotenv\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# Notebook imports\n",
    "import ast\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './dataset/'\n",
    "EVALS_DIR = './evals/'\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j GDS Link prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_NAME_PREFIX = 'ml_projection_'\n",
    "\n",
    "NEO4J_URI = os.environ.get('NEO4J_URI')\n",
    "NEO4J_USER = os.environ.get('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD')\n",
    "\n",
    "TEST_FRACTION = 0.3\n",
    "TRAIN_FRACTION = 0.6\n",
    "MIN_EPOCHS = 10\n",
    "MAX_EPOCHS = 100\n",
    "VALIDATION_FOLDS = 5  # used to tune hyperparameters and determine best model\n",
    "NEGATIVE_SAMPLING_RATIO = 1 # (q - r) / r, where q = n(n-1)/2, n = number of nodes, r = actual number of edges\n",
    "NEGATIVE_CLASS_WEIGHT = 1# / NEGATIVE_SAMPLING_RATIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gds_client():\n",
    "    return GraphDataScience(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "def get_projection_name(version=1):\n",
    "    return PROJECTION_NAME_PREFIX + str(version)\n",
    "\n",
    "\n",
    "def get_projection(gds, projection_name, for_predictions=False):\n",
    "    if gds.graph.exists(projection_name)['exists']:\n",
    "        return gds.graph.get(projection_name)\n",
    "    \n",
    "    pangenome_nodes = pd.read_csv(DATASET_DIR + 'pangenome_nodes.csv')\n",
    "    taxon_nodes = pd.read_csv(DATASET_DIR + 'taxon_nodes.csv')\n",
    "\n",
    "    # prune taxonomy tree to only include viruses\n",
    "    taxon_nodes = taxon_nodes.loc[\n",
    "        taxon_nodes['taxKingdom'] == 'Viruses'\n",
    "    ]\n",
    "    taxon_nodes['nodeLabels'] = [\n",
    "        ['Taxon', 'TargetRank'] \n",
    "        if x == 'order' \n",
    "        else 'Taxon' \n",
    "        for x in taxon_nodes['rank']\n",
    "    ]\n",
    "\n",
    "    # assert there's no collision of nodeIds from diffferent DB instances\n",
    "    assert len(set(pangenome_nodes['nodeId'].values).intersection(set(taxon_nodes['nodeId'].values))) == 0\n",
    "\n",
    "    nodes = pd.concat([pangenome_nodes, taxon_nodes])\n",
    "    nodes['labels'] = nodes['nodeLabels']\n",
    "    # prune segmeneted fasta nodes\n",
    "    nodes = nodes.loc[\n",
    "        (nodes['labels'] != 'Fasta') |\n",
    "        (nodes['isSegmented'] == False)\n",
    "    ]\n",
    "    nodes = nodes[[\n",
    "        'nodeId',\n",
    "        'labels',\n",
    "        'degree',\n",
    "    ]]\n",
    "\n",
    "    pangenome_rels = pd.read_csv(DATASET_DIR + 'pangenome_edges.csv')\n",
    "    taxonomy_rels = pd.read_csv(DATASET_DIR + 'taxon_has_parent_edges.csv')\n",
    "    taxonomy_rels = taxonomy_rels.loc[\n",
    "        taxonomy_rels['sourceNodeId'].isin(taxon_nodes['nodeId'].values) &\n",
    "        taxonomy_rels['targetNodeId'].isin(taxon_nodes['nodeId'].values)\n",
    "    ]\n",
    "\n",
    "    pangenome_taxonomy_rels = pd.read_csv(DATASET_DIR + 'fasta_has_tax_order_edges.csv')\n",
    "    relationships = pd.concat([\n",
    "        pangenome_rels,\n",
    "        taxonomy_rels,\n",
    "        pangenome_taxonomy_rels,\n",
    "    ])\n",
    "\n",
    "    relationships = relationships[[\n",
    "        'sourceNodeId',\n",
    "        'targetNodeId',\n",
    "        'relationshipType',\n",
    "        'weight'\n",
    "    ]]\n",
    "\n",
    "    undirected_relationship_types = relationships['relationshipType'].unique().tolist()\n",
    "    if not for_predictions:\n",
    "        # undirected_relationship_types.remove('hasTaxOrder')\n",
    "        undirected_relationship_types.remove('hasHit')\n",
    "        undirected_relationship_types.remove('hasRegion')\n",
    "        # undirected_relationship_types.remove('hasAffiliate')\n",
    "        undirected_relationship_types.remove('hasDownstream')\n",
    "        # undirected_relationship_types.remove('hasMember')\n",
    "        \n",
    "    projection = gds.graph.construct(\n",
    "        graph_name=projection_name,\n",
    "        nodes=nodes,\n",
    "        relationships=relationships,\n",
    "        concurrency=4,\n",
    "        undirected_relationship_types=undirected_relationship_types,\n",
    "    )\n",
    "    return projection\n",
    "\n",
    "def delete_projection(gds, projection_name):\n",
    "    if gds.graph.exists(projection_name)['exists']:\n",
    "        gds.graph.drop(gds.graph.get(projection_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "projection_name = get_projection_name(version)\n",
    "gds = get_gds_client()\n",
    "delete_projection(gds, projection_name)\n",
    "projection = get_projection(gds, projection_name)\n",
    "print(projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSage test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GraphSage cannot be trained in the GDS pipeline, and it doesn't seem to be possible to get the test split from the gds pipeline.\n",
    "- To avoid info leakeage, we'd need to create a seperate dataset to train graphsage.\n",
    "- We can also consider resampling the dataset to balance each of the taxonomy orders with varying number of hits to produce better embeddings.\n",
    "- Running graphSage on the current neo4j instance causes it to crash, will try this pipeline in pytorch on a high-memory instance later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gds.alpha.ml.splitRelationships.mutate(\n",
    "    G=gds.graph.get(projection_name),\n",
    "    sourceNodeLabels=['Fasta'],\n",
    "    targetNodeLabels=['TargetRank'],\n",
    "    relationshipTypes=['hasTaxOrder'],\n",
    "    holdoutFraction=TEST_FRACTION,\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    holdoutRelationshipType='hasTaxOrder_TEST',\n",
    "    remainingRelationshipType='hasTaxOrder_TRAIN',\n",
    "    negativeSamplingRatio=NEGATIVE_SAMPLING_RATIO,\n",
    "    relationshipWeightProperty='weight',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "gds.fastRP.mutate(\n",
    "    G=gds.graph.get(projection_name),\n",
    "    mutateProperty='fastRP',\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    # relationshipWeightProperty='weight',\n",
    "    # featureProperties=[\"degree\"],\n",
    "    nodeLabels=[\n",
    "        'Taxon',\n",
    "        'Fasta',\n",
    "        'Hit',\n",
    "        'HitFamily',\n",
    "        'HitRegion',\n",
    "    ],\n",
    "    relationshipTypes=[\n",
    "        # \"*\",\n",
    "        'hasTaxOrder_TRAIN',\n",
    "        'HAS_PARENT',\n",
    "        'hasHit',\n",
    "        'hasRegion',\n",
    "        'hasAffiliate',\n",
    "        'hasMember',\n",
    "        'hasDownstream',\n",
    "    ],\n",
    "    embeddingDimension=128,\n",
    "    normalizationStrength=-0.5,\n",
    "    # nodeSelfInfluence=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gds.beta.model.exists('graphSage')['exists']:\n",
    "    gds.beta.model.drop(gds.model.get('graphSage'))\n",
    "\n",
    "gds.beta.graphSage.train(\n",
    "    G=gds.graph.get(projection_name),\n",
    "    modelName='graphSage',\n",
    "    featureProperties=['fastRP'],\n",
    "    nodeLabels=[\n",
    "        'Taxon',\n",
    "        'Fasta',\n",
    "        'Hit',\n",
    "        'HitFamily',\n",
    "        'HitRegion',\n",
    "    ],\n",
    "    relationshipTypes=[\n",
    "        # '*'\n",
    "        'hasTaxOrder_TEST',\n",
    "        'HAS_PARENT',\n",
    "        'hasHit',\n",
    "        'hasRegion',\n",
    "        'hasAffiliate',\n",
    "        'hasMember',\n",
    "        'hasDownstream',\n",
    "    ],\n",
    "    embeddingDimension=256,\n",
    "    epochs=10,\n",
    "    searchDepth=10,\n",
    "    # relationshipWeightProperty='weight',\n",
    "    # featureProperties=['fastRP', 'degree'],\n",
    "    # featureProperties=['degree'],\n",
    "    # featureProperties=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_sage_embeddings = gds.beta.graphSage.stream(\n",
    "#     G=gds.graph.get(projection_name),\n",
    "#     modelName='graphSage',\n",
    "#     nodeLabels=[\n",
    "#         'Taxon',\n",
    "#         'Fasta',\n",
    "#         'Hit',\n",
    "#         'HitFamily',\n",
    "#         'HitRegion',\n",
    "#     ],\n",
    "#     relationshipTypes=['*'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-eval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "\n",
    "pipeline_name = 'taxon_lp_pipeline'\n",
    "model_encoder = 'fastRP'\n",
    "\n",
    "if gds.beta.pipeline.exists(pipeline_name)['exists']:\n",
    "    gds.beta.pipeline.drop(gds.pipeline.get(pipeline_name))\n",
    "\n",
    "pipeline, _ = gds.beta.pipeline.linkPrediction.create(pipeline_name)\n",
    "\n",
    "G = gds.graph.get(projection_name)\n",
    "\n",
    "pipeline.addNodeProperty(\n",
    "    procedure_name=model_encoder,\n",
    "    mutateProperty=model_encoder,\n",
    "    embeddingDimension=384, # 384\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    # featureProperties=[\"features\"],\n",
    "    relationshipWeightProperty='weight',\n",
    "    normalizationStrength=-0.5,\n",
    "    # nodeSelfInfluence=2,\n",
    "    contextRelationshipTypes=[\n",
    "        \"HAS_PARENT\",\n",
    "        \"hasHit\",\n",
    "        \"hasRegion\",\n",
    "        \"hasAffiliate\",\n",
    "        \"hasMember\",\n",
    "        \"hasDownstream\",\n",
    "    ],\n",
    "    contextNodeLabels=[\n",
    "        \"Taxon\",\n",
    "        \"Fasta\",\n",
    "        \"Hit\",\n",
    "        \"HitRegion\",\n",
    "        \"HitFamily\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# L2, HADAMARD, COSINE, SAME_CATEGORY\n",
    "pipeline.addFeature(\"hadamard\", nodeProperties=[model_encoder])\n",
    "\n",
    "# pipeline.addNodeProperty(\n",
    "#     procedure_name=\"beta.graphSage\",\n",
    "#     mutateProperty=\"graphSage\",\n",
    "#     modelName=\"graphSage\",\n",
    "#     # featureProperties=[\"fastRP\"],\n",
    "#     contextRelationshipTypes=[\n",
    "#         \"HAS_PARENT\",\n",
    "#         \"hasHit\",\n",
    "#         \"hasRegion\",\n",
    "#         \"hasAffiliate\",\n",
    "#         \"hasMember\",\n",
    "#         \"hasDownstream\",\n",
    "#     ],\n",
    "#     contextNodeLabels=[\n",
    "#         \"Taxon\",\n",
    "#         \"Fasta\",\n",
    "#         \"Hit\",\n",
    "#         \"HitRegion\",\n",
    "#         \"HitFamily\",\n",
    "#     ],\n",
    "#     # relationshipWeightProperty=\"weight\",\n",
    "# )\n",
    "\n",
    "# pipeline.addFeature(\"hadamard\", nodeProperties=[\"graphSage\"])\n",
    "\n",
    "split = pipeline.configureSplit(\n",
    "    testFraction=TEST_FRACTION,\n",
    "    validationFolds=VALIDATION_FOLDS,\n",
    "    negativeSamplingRatio=NEGATIVE_SAMPLING_RATIO,\n",
    ")\n",
    "\n",
    "pipeline.addLogisticRegression(penalty=(0.1, 2))\n",
    "# pipeline.addLogisticRegression(maxEpochs=200, penalty=(0.0, 2))\n",
    "pipeline.addRandomForest(maxDepth=(2, 50))\n",
    "# pipeline.addRandomForest(maxDepth=(2, 100))\n",
    "\n",
    "# Last layer should match number of classes being predicted\n",
    "# pipeline.addMLP(hiddenLayerSizes=[16, 16, 4], penalty=1, patience=2)\n",
    "# pipeline.addMLP(hiddenLayerSizes=[128, 64, 2], maxEpochs=100, patience=10)\n",
    "# [128, 256, 128, 2]\n",
    "# [256, 128, 2] *\n",
    "# [256, 384, 256, 128, 2]\n",
    "pipeline.addMLP(\n",
    "    # hiddenLayerSizes=[256, 128, 2], #[384, 128, 2],\n",
    "    hiddenLayerSizes=[384, 128, 2],\n",
    "    maxEpochs=100,\n",
    "    patience=10,\n",
    "    focusWeight=0.5,\n",
    "    # focusWeight={'range': [0.0, 0.5]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the train and eval pipeline\n",
    "\n",
    "if gds.beta.model.exists(model_encoder)['exists']:\n",
    "    gds.beta.model.drop(gds.model.get(model_encoder))\n",
    "\n",
    "model, evals = pipeline.train(\n",
    "    pipeline=pipeline,\n",
    "    G=gds.graph.get(projection_name),\n",
    "    modelName=model_encoder,\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    targetRelationshipType='hasTaxOrder',\n",
    "    sourceNodeLabel='Fasta',\n",
    "    targetNodeLabel='TargetRank',\n",
    "    negativeClassWeight=NEGATIVE_CLASS_WEIGHT,\n",
    "    metrics=[ \n",
    "        'AUCPR'\n",
    "    ],\n",
    ")\n",
    "\n",
    "evals.to_csv(EVALS_DIR + 'lp_gds_evals.csv')\n",
    "display(evals['modelInfo']['bestParameters'])\n",
    "\n",
    "print('-----------------')\n",
    "\n",
    "print('Best model metrics (general):')\n",
    "print('AUCPR:')\n",
    "display(evals['modelInfo']['metrics']['AUCPR'])\n",
    "\n",
    "# {'test': 0.8079789288270317,\n",
    "#  'validation': {'min': 0.7552969944662513,\n",
    "#   'max': 0.7978044070290363,\n",
    "#   'avg': 0.7801199361717549},\n",
    "#  'outerTrain': 0.99993761244742,\n",
    "#  'train': {'min': 0.9999357007540141,\n",
    "#   'max': 0.9999999999999999,\n",
    "#   'avg': 0.999974082447445}}\n",
    "\n",
    "# {'test': 0.7165068713762617,\n",
    "#  'validation': {'min': 0.6341140732932966,\n",
    "#   'max': 0.7483135387115906,\n",
    "#   'avg': 0.6846875847262417},\n",
    "#  'outerTrain': 0.9999455163442132,\n",
    "#  'train': {'min': 0.9999510657293991,\n",
    "#   'max': 0.9999836847323984,\n",
    "#   'avg': 0.9999713434218448}}\n",
    "\n",
    "# {'test': 0.9886814718381982,\n",
    "#  'validation': {'min': 0.985947330770067,\n",
    "#   'max': 0.9909918266054518,\n",
    "#   'avg': 0.9887788612948111},\n",
    "#  'outerTrain': 0.9989832669230688,\n",
    "#  'train': {'min': 0.9989039358314438,\n",
    "#   'max': 0.998947612674645,\n",
    "#   'avg': 0.9989210279491374}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "\n",
    "projection_name = get_projection_name(version)\n",
    "gds = get_gds_client()\n",
    "delete_projection(gds, projection_name)\n",
    "projection = get_projection(gds, projection_name, for_predictions=True)\n",
    "print(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map nodeIds to appIds\n",
    "\n",
    "def enrich_app_data(df):\n",
    "    pangenome_nodes = pd.read_csv(DATASET_DIR + 'pangenome_nodes.csv')\n",
    "    fasta_nodes = pangenome_nodes.loc[\n",
    "        pangenome_nodes['nodeLabels'] == 'Fasta'\n",
    "    ]\n",
    "    fasta_nodes = fasta_nodes[[\n",
    "        'nodeId',\n",
    "        'appId',\n",
    "        'order',\n",
    "    ]]\n",
    "    taxon_nodes = pd.read_csv(DATASET_DIR + 'taxon_nodes.csv')\n",
    "    taxon_nodes = taxon_nodes.loc[\n",
    "        taxon_nodes['taxKingdom'] == 'Viruses'\n",
    "    ]\n",
    "    taxon_nodes = taxon_nodes[[\n",
    "        'nodeId',\n",
    "        'appId',\n",
    "        'taxOrder',\n",
    "        'rank',\n",
    "    ]]\n",
    "\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        fasta_nodes,\n",
    "        how='left',\n",
    "        left_on='sourceNodeId',\n",
    "        right_on='nodeId',\n",
    "    )\n",
    "    df.drop('nodeId', axis=1, inplace=True)\n",
    "    df.rename(columns={'appId': 'sourceAppId', 'order': 'expectedOrder'}, inplace=True)\n",
    "\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        taxon_nodes,\n",
    "        how='left',\n",
    "        left_on='targetNodeId',\n",
    "        right_on='nodeId',\n",
    "    )\n",
    "    df.drop('nodeId', axis=1, inplace=True)\n",
    "    df.rename(columns={'appId': 'targetAppId', 'taxOrder': 'predictedOrder', 'rank': 'predictedRank'}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def format_predictions(predictions):\n",
    "    predictions = predictions.sort_values(['node1', 'probability'], ascending=[True, False])\n",
    "    predictions['sourceNodeId'] = predictions['node1']\n",
    "    predictions.drop('node1', axis=1, inplace=True)\n",
    "    predictions['targetNodeId'] = predictions['node2']\n",
    "    predictions.drop('node2', axis=1, inplace=True)\n",
    "\n",
    "    merged = enrich_app_data(predictions)\n",
    "    # merged.dropna(inplace=True)\n",
    "\n",
    "    if merged['sourceAppId'].isna().sum() > 0:\n",
    "        non_null = merged.loc[\n",
    "            ~merged['sourceAppId'].isna()\n",
    "        ]\n",
    "        reversed_direction = merged.loc[\n",
    "            merged['sourceAppId'].isna()\n",
    "        ]\n",
    "        reversed_direction[['sourceNodeId','targetNodeId']] = reversed_direction[['targetNodeId','sourceNodeId']]\n",
    "        reversed_direction.drop([\n",
    "                'sourceAppId',\n",
    "                'targetAppId',\n",
    "                'expectedOrder',\n",
    "                'predictedOrder',\n",
    "                'predictedRank'\n",
    "            ], axis=1, inplace=True)\n",
    "        reversed_direction = enrich_app_data(reversed_direction)\n",
    "        merged = pd.concat([non_null, reversed_direction])\n",
    "\n",
    "    merged['correct'] = merged['expectedOrder'] == merged['predictedOrder']\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_exhaustive_predictions(model):\n",
    "    # Stream approximate predictions\n",
    "    predictions = model.predict_stream(\n",
    "        G=gds.graph.get(projection_name),\n",
    "        # topK=1,\n",
    "        topN=6318,\n",
    "        # randomSeed=RANDOM_SEED,\n",
    "        sourceNodeLabel='Fasta',\n",
    "        targetNodeLabel='TargetRank',\n",
    "        relationshipTypes=[\n",
    "            # \"hasTaxOrder\",\n",
    "            \"HAS_PARENT\",\n",
    "            \"hasHit\",\n",
    "            \"hasRegion\",\n",
    "            \"hasAffiliate\",\n",
    "            \"hasMember\",\n",
    "            \"hasDownstream\",\n",
    "        ],\n",
    "        sampleRate=1, # 1 = exhaustive\n",
    "        # threshold=0.5,\n",
    "        # randomJoins=2,\n",
    "        # maxIterations=3,\n",
    "        # deltaThreshold=0.01,\n",
    "    )\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def stream_approximate_predictions(model):\n",
    "    predictions = model.predict_stream(\n",
    "        G=gds.graph.get(projection_name),\n",
    "        topK=1,\n",
    "        # randomSeed=RANDOM_SEED,\n",
    "        sourceNodeLabel='Fasta',\n",
    "        targetNodeLabel='TargetRank',\n",
    "        relationshipTypes=[\n",
    "            # \"hasTaxOrder\",\n",
    "            \"HAS_PARENT\",\n",
    "            \"hasHit\",\n",
    "            \"hasRegion\",\n",
    "            \"hasAffiliate\",\n",
    "            \"hasMember\",\n",
    "            \"hasDownstream\",\n",
    "        ],\n",
    "        sampleRate=0.1, # 1 = exhaustive\n",
    "        # threshold=0.5,\n",
    "        # randomJoins=2,\n",
    "        # maxIterations=3,\n",
    "        # deltaThreshold=0.01,\n",
    "    )\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gds.model.get('fastRP')\n",
    "# preds_raw = stream_exhaustive_predictions(model)\n",
    "preds_raw = stream_approximate_predictions(model)\n",
    "preds = format_predictions(preds_raw)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv(DATASET_DIR + 'lp_gds_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds['sourceAppId'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds['probability'].describe())\n",
    "print(preds_raw['node1'].nunique())\n",
    "print(preds['sourceNodeId'].nunique())\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds['correct'].value_counts())\n",
    "# correct\n",
    "# False    61282\n",
    "# True      1038\n",
    "\n",
    "# group by source and check if any targets are correct\n",
    "grouped = preds.groupby('sourceAppId')\n",
    "grouped_correct = grouped['correct'].any()\n",
    "print(grouped_correct.value_counts())\n",
    "\n",
    "# correct\n",
    "# False    5372\n",
    "# True      788\n",
    "\n",
    "# False    5264\n",
    "# True      968\n",
    "\n",
    "# False    5194\n",
    "# True     1038\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.drop(get_projection_name())\n",
    "gds.beta.model.drop(gds.model.get(model_encoder))\n",
    "gds.beta.pipeline.drop(gds.pipeline.get(pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda device before imports\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric import seed_everything\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.utils import to_networkx, degree, mask_select, dropout_edge\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GCNConv, to_hetero, MIPSKNNIndex, BatchNorm, GraphSAGE\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.metrics import (\n",
    "    LinkPredMAP,\n",
    "    LinkPredPrecision,\n",
    "    LinkPredRecall,\n",
    ")\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config vars\n",
    "MODEL_DIR = './models/'\n",
    "\n",
    "NEGATIVE_SAMPLING_RATIO = 2\n",
    "TEST_FRACTION = 0.2\n",
    "VAL_FRACTION = 0\n",
    "TRAIN_FRACTION = (1 - TEST_FRACTION - VAL_FRACTION) \n",
    "MAX_EPOCHS = 100\n",
    "MIN_EPOCHS = 10\n",
    "BATCH_SIZE = 1024 # 1024, 2048, 4096\n",
    "NUM_NEIGHBORS = [-1, -1, -1, -1] # [-1, -1, -1, -1]\n",
    "LEARNING_RATE = 0.001\n",
    "L2_REG = 0.0001 \n",
    "DROPOUT_RATE = 0.6\n",
    "\n",
    "TARGET_RANK = 'order'\n",
    "TARGET_EDGE = ('Fasta', 'hasTaxRank', 'TargetTaxon')\n",
    "# TARGET_EDGE = ('HitRegion', 'hasDownstream', 'HitRegion')\n",
    "\n",
    "COLLAPSE_TAXONOMY_GRAPH = True\n",
    "COLLAPSE_PANGENOME_GRAPH = True\n",
    "USE_REPRESENTATIVE_HITS = True\n",
    "USE_RESAMPLED_DATASET = True\n",
    "USE_METAPATHS = True\n",
    "USE_HAS_HOST = False\n",
    "\n",
    "USE_ESM_EMBEDDINGS = False\n",
    "USE_LLM_EMBEDDINGS = False\n",
    "USE_UNSUPERVISED_MODEL = True\n",
    "USE_HIERARCHICAL_LOSS = False\n",
    "\n",
    "seed_everything(RANDOM_SEED)\n",
    "\n",
    "# device  = torch.device('cpu')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "def clear_cuda_mem():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "clear_cuda_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base pytorch geometric graph\n",
    "\n",
    "# Dataframe encoders \n",
    "class IdentityEncoder(object):\n",
    "    # The 'IdentityEncoder' takes the raw column values and converts them to\n",
    "    # PyTorch tensors.\n",
    "    def __init__(self, dtype=None, is_list=False, is_tensor=False):\n",
    "        self.dtype = dtype\n",
    "        self.is_list = is_list\n",
    "        self.is_tensor = is_tensor\n",
    "\n",
    "    def __call__(self, df):\n",
    "        if self.is_tensor:\n",
    "            if self.is_list:\n",
    "                return torch.stack([torch.tensor([el]) for el in df.values])\n",
    "            return torch.from_numpy(df.values).to(self.dtype)\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "class ListEncoder(object):\n",
    "    def __init__(self, sep=',', is_tensor=False):\n",
    "        self.sep = sep\n",
    "        self.is_tensor = is_tensor\n",
    "\n",
    "    def __call__(self, df):\n",
    "        eval_df = df.apply(\n",
    "            lambda x: [val for val in ast.literal_eval(x)])\n",
    "        return torch.stack([torch.tensor(el) for el in eval_df.values])\n",
    "\n",
    "class LabelEncoder(object):\n",
    "    # The 'LabelEncoder' splits the raw column strings by 'sep' and converts\n",
    "    # individual elements to categorical labels.\n",
    "    def __init__(self, sep=',', is_tensor=False, mapping=None):\n",
    "        self.sep = sep\n",
    "        self.is_tensor = is_tensor\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __call__(self, df):\n",
    "        mapping = self.mapping\n",
    "        if self.is_tensor:\n",
    "            if not mapping:\n",
    "                labels = set(\n",
    "                    label for col in df.values\n",
    "                    for label in col.split(self.sep)\n",
    "                )\n",
    "                mapping = {label: i for i, label in enumerate(labels)}\n",
    "            x = torch.zeros(len(df), len(mapping))\n",
    "            for i, col in enumerate(df.values):\n",
    "                for label in col.split(self.sep):\n",
    "                    x[i, mapping[label]] = 1\n",
    "            return x\n",
    "        if not mapping:\n",
    "            labels = df[df.columns[0]].unique()\n",
    "            mapping = {label: i for i, label in enumerate(labels)}\n",
    "        return df.replace(mapping)\n",
    "\n",
    "\n",
    "def load_node_tensor(df, index_col, encoders=None):\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "    x = torch.ones(size=(len(df.index), 1))\n",
    "\n",
    "    if encoders is not None:\n",
    "        xs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "        x = x.float()\n",
    "    \n",
    "    assert len(mapping) == len(df.index.unique())\n",
    "    return x, mapping\n",
    "\n",
    "def load_edge_tensor(df, src_index_col, src_mapping,\n",
    "                     dst_index_col, dst_mapping, encoders=None):\n",
    "    \n",
    "    src = [src_mapping[index] for index in df[src_index_col]]\n",
    "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
    "    edge_index = torch.tensor([src, dst])\n",
    "    edge_attr = None\n",
    "    if encoders is not None:\n",
    "        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        edge_attr = torch.cat(edge_attrs, dim=-1)\n",
    "\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def get_virus_taxon_nodes():\n",
    "    taxon_nodes_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}taxon_nodes.csv',\n",
    "        index_col='appId',\n",
    "        header=0,\n",
    "    )\n",
    "    taxon_nodes_df = taxon_nodes_df.loc[\n",
    "        taxon_nodes_df['taxKingdom'] == 'Viruses'\n",
    "    ]\n",
    "\n",
    "    ## TMP: remove target taxons that are not in dataset\n",
    "    # pangenome_nodes_df = get_filtered_pangenome_nodes()\n",
    "    # fasta_nodes = pangenome_nodes_df.loc[\n",
    "    #     pangenome_nodes_df['nodeLabels'] == 'Fasta'\n",
    "    # ]\n",
    "    # target_rank = f\"tax{TARGET_RANK.capitalize()}\"\n",
    "    # taxon_nodes_df = taxon_nodes_df.loc[\n",
    "    #     (taxon_nodes_df['rank'] != TARGET_RANK) | \n",
    "    #     (taxon_nodes_df[target_rank].isin(fasta_nodes[TARGET_RANK].values))\n",
    "    # ]\n",
    "\n",
    "    return taxon_nodes_df\n",
    "\n",
    "def get_host_taxon_nodes():\n",
    "    taxon_nodes_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}taxon_nodes.csv',\n",
    "        index_col='appId',\n",
    "        header=0,\n",
    "    )\n",
    "    fasta_has_host = pd.read_csv(\n",
    "        f'{DATASET_DIR}fasta_has_host_gb_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    fasta_has_host['appId'] = fasta_has_host['targetAppId'].astype(int)\n",
    "\n",
    "    host_taxon_nodes_df = pd.merge(\n",
    "        fasta_has_host['appId'],\n",
    "        taxon_nodes_df,\n",
    "        how='left',\n",
    "        on='appId',\n",
    "    )\n",
    "\n",
    "    return host_taxon_nodes_df\n",
    "\n",
    "def get_filtered_pangenome_nodes():\n",
    "    pangenome_nodes_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}pangenome_nodes.csv',\n",
    "        index_col='appId',\n",
    "        header=0,\n",
    "    )\n",
    "    ## prune segmeneted fasta nodes\n",
    "    pangenome_nodes_df = pangenome_nodes_df.loc[\n",
    "        (pangenome_nodes_df['nodeLabels'] != 'Fasta') |\n",
    "        (pangenome_nodes_df['isSegmented'] == False)\n",
    "    ]\n",
    "\n",
    "    # TMP: only include fasta nodes matching provided orders\n",
    "    # pangenome_nodes_df = pangenome_nodes_df.loc[\n",
    "    #     (pangenome_nodes_df['nodeLabels'] != 'Fasta') |\n",
    "    #     (pangenome_nodes_df['order'].isin([\n",
    "    #         'Wolframvirales', 'Goujianvirales', 'Muvirales', 'Mindivirales',\n",
    "    #     'Yadokarivirales', 'Serpentovirales', 'Stellavirales', 'Nodamuvirales',\n",
    "    #         'Hepelivirales', 'Cryppavirales'\n",
    "    #     ]))\n",
    "    # ]\n",
    "\n",
    "    if USE_RESAMPLED_DATASET:\n",
    "        resampled_fastas = pd.read_csv(\n",
    "            f'{DATASET_DIR}fasta_resampled_fixed.csv',\n",
    "        )\n",
    "        pangenome_nodes_df = pangenome_nodes_df.loc[\n",
    "            (pangenome_nodes_df['nodeLabels'] != 'Fasta') |\n",
    "            (pangenome_nodes_df['accession'].isin(resampled_fastas['accession']))\n",
    "        ]\n",
    "\n",
    "        # TMP: optionally remove all associated nodes\n",
    "        # filtered_names = tuple(resampled_fastas['name'].values)\n",
    "        # pangenome_nodes_df = pangenome_nodes_df.loc[\n",
    "        #     (pangenome_nodes_df['nodeLabels'] != 'Hit') |\n",
    "        #     (~pangenome_nodes_df['name'].str.startswith(filtered_names))\n",
    "        # ]\n",
    "        # pangenome_nodes_df = pangenome_nodes_df.loc[\n",
    "        #     (pangenome_nodes_df['nodeLabels'] != 'HitRegion') |\n",
    "        #     (~pangenome_nodes_df['name'].str.startswith(filtered_names))\n",
    "        # ]\n",
    "\n",
    "        # TMP remove domains with high degree (resampled_domains)\n",
    "        domain_fasta_counts = pd.read_csv(DATASET_DIR + 'domain_fasta_counts.csv')\n",
    "        resampled_domains = domain_fasta_counts.loc[\n",
    "            domain_fasta_counts['fastaCount'] < 2000 # 3092\n",
    "        ]\n",
    "        pangenome_nodes_df = pangenome_nodes_df.loc[\n",
    "            (pangenome_nodes_df['nodeLabels'] != 'HitFamily') |\n",
    "            (pangenome_nodes_df['accession'].isin(resampled_domains['domainAccession']))\n",
    "        ]\n",
    "\n",
    "    return pangenome_nodes_df\n",
    "\n",
    "def create_pyg_graph():\n",
    "    data = HeteroData()\n",
    "    mappings = {}\n",
    "\n",
    "    ## Load taxons node tensors\n",
    "    taxon_nodes_df = get_virus_taxon_nodes()\n",
    "    pangenome_nodes_df = get_filtered_pangenome_nodes()\n",
    "\n",
    "    target_taxons = taxon_nodes_df.loc[\n",
    "        taxon_nodes_df['rank'] == TARGET_RANK\n",
    "    ]\n",
    "    target_taxon_x, target_taxon_mapping = load_node_tensor(\n",
    "        df=target_taxons,\n",
    "        index_col='appId',\n",
    "        # encoders={\n",
    "        #     'fastrp': ListEncoder(is_tensor=True),\n",
    "        # }\n",
    "    )\n",
    "    data['TargetTaxon'].node_id = torch.arange(len(target_taxon_mapping))\n",
    "    # data['TargetTaxon'].x = target_taxon_x\n",
    "    mappings['TargetTaxon'] = target_taxon_mapping\n",
    "\n",
    "    ## Load pangenome node tensors \n",
    "    for node_type in ['Fasta', 'Hit', 'HitRegion', 'HitFamily']:\n",
    "        node_df = pangenome_nodes_df.loc[\n",
    "            pangenome_nodes_df['nodeLabels'] == node_type\n",
    "        ]\n",
    "        x, mapping = load_node_tensor(\n",
    "            df=node_df,\n",
    "            index_col='appId',\n",
    "        )\n",
    "        data[node_type].node_id = torch.arange(len(mapping))\n",
    "        # data[node_type].x = x\n",
    "        mappings[node_type] = mapping\n",
    "\n",
    "    ## Load pangenome edge tensors\n",
    "    pangenome_edges_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}pangenome_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    # remove edges that are not in the pruned pangenome graph\n",
    "    pangenome_edges_df = pangenome_edges_df.loc[\n",
    "        pangenome_edges_df['sourceAppId'].isin(pangenome_nodes_df.index) &\n",
    "        pangenome_edges_df['targetAppId'].isin(pangenome_nodes_df.index)\n",
    "    ]\n",
    "    if USE_REPRESENTATIVE_HITS:\n",
    "        representative_has_affiliates = pd.read_csv(DATASET_DIR + 'pangenome_representatives.csv')\n",
    "        pangenome_edges_df = pangenome_edges_df.loc[\n",
    "            (pangenome_edges_df['relationshipType'] != 'hasAffiliate') |\n",
    "            (\n",
    "                (pangenome_edges_df['sourceAppId'].isin(representative_has_affiliates['sourceAppId'])) &\n",
    "                (pangenome_edges_df['targetAppId'].isin(representative_has_affiliates['targetAppId']))\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    config = {\n",
    "        'hasMember': {\n",
    "            'src': 'HitFamily',\n",
    "            'dst': 'Hit',\n",
    "        },\n",
    "        'hasHit': {\n",
    "            'src': 'Fasta',\n",
    "            'dst': 'Hit',\n",
    "        },\n",
    "        'hasRegion': {\n",
    "            'src': 'Fasta',\n",
    "            'dst': 'HitRegion',\n",
    "        },\n",
    "        'hasAffiliate': {\n",
    "            'src': 'HitRegion',\n",
    "            'dst': 'Hit',\n",
    "        },\n",
    "        'hasDownstream': {\n",
    "            'src': 'HitRegion',\n",
    "            'dst': 'HitRegion',\n",
    "        },\n",
    "    }\n",
    "    for rel_type, rel_config in config.items():\n",
    "        edge_df = pangenome_edges_df.loc[\n",
    "            pangenome_edges_df['relationshipType'] == rel_type\n",
    "        ]\n",
    "        encoders = {\n",
    "                'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        } if rel_type != TARGET_EDGE[1] else None\n",
    "\n",
    "        edge_index, edge_label = load_edge_tensor(\n",
    "            df=edge_df,\n",
    "            src_index_col='sourceAppId',\n",
    "            src_mapping=mappings[rel_config['src']],\n",
    "            dst_index_col='targetAppId',\n",
    "            dst_mapping=mappings[rel_config['dst']],\n",
    "            encoders=encoders,\n",
    "        )\n",
    "        data[rel_config['src'], rel_type, rel_config['dst']].edge_index = edge_index\n",
    "        data[rel_config['src'], rel_type, rel_config['dst']].edge_label = edge_label\n",
    "\n",
    "    ## Load pangenome taxon edge tensors\n",
    "    pangenome_taxon_edges_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}fasta_has_tax_{TARGET_RANK}_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    ## remove edges that are not in the pruned pangenome graph or taxon tree\n",
    "    pangenome_taxon_edges_df = pangenome_taxon_edges_df.loc[\n",
    "        pangenome_taxon_edges_df['sourceAppId'].isin(pangenome_nodes_df.index) &\n",
    "        pangenome_taxon_edges_df['targetAppId'].isin(taxon_nodes_df.index)\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=pangenome_taxon_edges_df,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['Fasta'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['TargetTaxon'],\n",
    "    )\n",
    "    data['Fasta', 'hasTaxRank', 'TargetTaxon'].edge_index = edge_index\n",
    "    data['Fasta', 'hasTaxRank', 'TargetTaxon'].edge_label = edge_label\n",
    "    \n",
    "    # delete fasta nodes with missing taxon edges\n",
    "    # for node_id in data['Fasta'].node_id:\n",
    "    #     if node_id not in edge_label_index[0]:\n",
    "\n",
    "    # transform = T.Compose([T.remove_isolated_nodes.RemoveIsolatedNodes()])\n",
    "    # data = transform(data)\n",
    "\n",
    "    return data, mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: Modify graph adjacencies\n",
    "\n",
    "def add_context_taxonomy_collapsed(data, mappings):\n",
    "    taxon_nodes_df = get_virus_taxon_nodes()\n",
    "    target_taxons = taxon_nodes_df.loc[\n",
    "        (taxon_nodes_df['rank'] == TARGET_RANK)\n",
    "    ]\n",
    "\n",
    "    # Lefavirales (Taxonomy ID: 2840070) has no phylum\n",
    "    target_taxons = target_taxons.loc[\n",
    "        ~target_taxons['taxPhylum'].isna()\n",
    "    ]\n",
    "    context_taxons = taxon_nodes_df.loc[\n",
    "        taxon_nodes_df['rank'].isin(['phylum', 'kingdom'])\n",
    "    ]\n",
    "    context_taxon_x, context_taxon_mapping = load_node_tensor(\n",
    "        df=context_taxons,\n",
    "        index_col='appId',\n",
    "        # encoders={\n",
    "        #     'fastrp': ListEncoder(is_tensor=True),\n",
    "        # }\n",
    "    )\n",
    "    data['ContextTaxon'].node_id = torch.arange(len(context_taxon_mapping))\n",
    "    # data['ContextTaxon'].x = context_taxon_x\n",
    "    mappings['ContextTaxon'] = context_taxon_mapping\n",
    "\n",
    "    order_to_phylum_edges = []\n",
    "    phylum_to_kingdom_edges = []\n",
    "    for _, target_taxon in target_taxons.iterrows():\n",
    "        # if target_taxon['taxId'] not in mappings['TargetTaxon']:\n",
    "        #     continue\n",
    "\n",
    "        phylum_tax_id = taxon_nodes_df.loc[\n",
    "            (taxon_nodes_df['taxPhylum'] == target_taxon['taxPhylum']) &\n",
    "            (taxon_nodes_df['rank'] == 'phylum')\n",
    "        ]['taxId'].values[0]\n",
    "        \n",
    "        kingdom_tax_id = taxon_nodes_df.loc[\n",
    "            (taxon_nodes_df['taxKingdom'] == target_taxon['taxKingdom']) &\n",
    "            (taxon_nodes_df['rank'] == 'kingdom')\n",
    "        ]['taxId'].values[0]\n",
    "\n",
    "        order_to_phylum_edges.append({\n",
    "            'sourceAppId': target_taxon['taxId'],\n",
    "            'targetAppId': phylum_tax_id,\n",
    "            'weight': 1\n",
    "        })\n",
    "\n",
    "        phylum_to_kingdom_edges.append({\n",
    "            'sourceAppId': phylum_tax_id,\n",
    "            'targetAppId': kingdom_tax_id,\n",
    "            'weight': 1,\n",
    "        })\n",
    "\n",
    "    order_to_phylum_edges = pd.DataFrame(order_to_phylum_edges).drop_duplicates()\n",
    "    phylum_to_kingdom_edges = pd.DataFrame(phylum_to_kingdom_edges).drop_duplicates()\n",
    "\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=order_to_phylum_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['TargetTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['ContextTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['TargetTaxon', 'hasParent', 'ContextTaxon'].edge_index = edge_index\n",
    "    data['TargetTaxon', 'hasParent', 'ContextTaxon'].edge_label = edge_label\n",
    "\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=phylum_to_kingdom_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['ContextTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['ContextTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['ContextTaxon', 'hasParent', 'ContextTaxon'].edge_index = edge_index\n",
    "    data['ContextTaxon', 'hasParent', 'ContextTaxon'].edge_label = edge_label\n",
    "\n",
    "    return data\n",
    "        \n",
    "def add_context_taxonomy(data, mappings):\n",
    "    taxon_nodes_df = get_virus_taxon_nodes()\n",
    "\n",
    "    context_taxons = taxon_nodes_df.loc[\n",
    "        (taxon_nodes_df['rank'] != TARGET_RANK) \n",
    "    ]\n",
    "\n",
    "    context_taxon_x, context_taxon_mapping = load_node_tensor(\n",
    "        df=context_taxons,\n",
    "        index_col='appId',\n",
    "        # encoders={\n",
    "        #     'fastrp': ListEncoder(is_tensor=True),\n",
    "        # },\n",
    "    )\n",
    "    data['ContextTaxon'].node_id = torch.arange(len(context_taxon_mapping)) \n",
    "    # data['ContextTaxon'].x = context_taxon_x\n",
    "    mappings['ContextTaxon'] = context_taxon_mapping\n",
    "\n",
    "\n",
    "    ## Load taxon edge tensors\n",
    "    taxon_edges = pd.read_csv(\n",
    "        f'{DATASET_DIR}taxon_has_parent_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    # remove edges that are not in the pruned taxon tree\n",
    "    taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(taxon_nodes_df.index) &\n",
    "        taxon_edges['targetAppId'].isin(taxon_nodes_df.index)\n",
    "    ]\n",
    "    context_taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(context_taxons.index) &\n",
    "        taxon_edges['targetAppId'].isin(context_taxons.index)\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=context_taxon_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['ContextTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['ContextTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['ContextTaxon', 'hasParent', 'ContextTaxon'].edge_index = edge_index\n",
    "    data['ContextTaxon', 'hasParent', 'ContextTaxon'].edge_label = edge_label\n",
    "\n",
    "    c_to_t_taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(mappings['ContextTaxon'].keys()) &\n",
    "        taxon_edges['targetAppId'].isin(mappings['TargetTaxon'].keys())\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=c_to_t_taxon_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['ContextTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['TargetTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['ContextTaxon', 'hasParent', 'TargetTaxon'].edge_index = edge_index\n",
    "    data['ContextTaxon', 'hasParent', 'TargetTaxon'].edge_label = edge_label\n",
    "\n",
    "    t_to_c_taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(mappings['TargetTaxon'].keys()) &\n",
    "        taxon_edges['targetAppId'].isin(mappings['ContextTaxon'].keys())\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=t_to_c_taxon_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['TargetTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['ContextTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['TargetTaxon', 'hasParent', 'ContextTaxon'].edge_index = edge_index\n",
    "    data['TargetTaxon', 'hasParent', 'ContextTaxon'].edge_label = edge_label\n",
    "    return data\n",
    "\n",
    "def add_context_has_host(data, mappings):\n",
    "    host_taxon_nodes = get_host_taxon_nodes()\n",
    "    host_taxon_x, host_taxon_mapping = load_node_tensor(\n",
    "        df=host_taxon_nodes,\n",
    "        index_col='appId',\n",
    "        # encoders={\n",
    "        #     'fastrp': ListEncoder(is_tensor=True),\n",
    "        # }\n",
    "    )\n",
    "    data['HostTaxon'].node_id = torch.arange(len(host_taxon_mapping))\n",
    "    # data['HostTaxon'].x = host_taxon_x\n",
    "    mappings['HostTaxon'] = host_taxon_mapping\n",
    "\n",
    "    # pangenome_nodes_df = get_filtered_pangenome_nodes()\n",
    "    inv_mappings = get_inv_mappings(mappings)\n",
    "    pangenome_nodes = [inv_mappings['Fasta'][node.item()] for node in data['Fasta'].node_id]\n",
    "    fasta_has_host = pd.read_csv(\n",
    "        f'{DATASET_DIR}fasta_has_host_gb_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    fasta_has_host['targetAppId'] = fasta_has_host['targetAppId'].astype(int)\n",
    "\n",
    "    ## remove edges that are not in the pruned pangenome graph or taxon tree\n",
    "    fasta_has_host = fasta_has_host.loc[\n",
    "        fasta_has_host['sourceAppId'].isin(pangenome_nodes) &\n",
    "        fasta_has_host['targetAppId'].isin(host_taxon_nodes.index)\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=fasta_has_host,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['Fasta'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['HostTaxon'],\n",
    "    )\n",
    "    data['Fasta', 'hasHostGenbank', 'HostTaxon'].edge_index = edge_index\n",
    "    data['Fasta', 'hasHostGenbank', 'HostTaxon'].edge_label = edge_label\n",
    "    return data\n",
    "\n",
    "def collapse_pangenome_graph(data):\n",
    "    new_data = HeteroData()\n",
    "    filter_node_types = [\n",
    "        'HitRegion',\n",
    "        # 'HitFamily',\n",
    "        # 'Hit',\n",
    "    ]\n",
    "    filter_edge_types = [\n",
    "        ('Fasta', 'hasRegion', 'HitRegion'),\n",
    "        ('HitRegion', 'rev_hasRegion', 'Fasta'),\n",
    "        ('HitRegion', 'hasAffiliate', 'Hit'),\n",
    "        ('Hit', 'rev_hasAffiliate', 'HitRegion'),\n",
    "        # ('HitFamily', 'hasMember', 'Hit'),\n",
    "        # ('Hit', 'rev_hasMember', 'HitFamily'),\n",
    "        ('HitRegion', 'hasDownstream', 'HitRegion'),\n",
    "        ('HitRegion', 'rev_hasDownstream', 'HitRegion'),\n",
    "    ]\n",
    "\n",
    "    for node_type in data.node_types:\n",
    "        if node_type in filter_node_types:\n",
    "            continue\n",
    "        new_data[node_type].node_id = data[node_type].node_id\n",
    "        if 'x' in data[node_type]:\n",
    "            new_data[node_type].x = data[node_type].x\n",
    "\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type in filter_edge_types:\n",
    "            continue\n",
    "        if 'edge_index' in data[edge_type]:\n",
    "            new_data[edge_type].edge_index = data[edge_type].edge_index\n",
    "        if 'edge_label' in data[edge_type]:\n",
    "            new_data[edge_type].edge_label = data[edge_type].edge_label\n",
    "        if 'edge_label_index' in data[edge_type]:\n",
    "            new_data[edge_type].edge_label_index = data[edge_type].edge_label_index\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def add_train_only_metapaths(data, train_metapaths=None):\n",
    "    if train_metapaths:\n",
    "        data['Fasta', 'metapath_0', 'Fasta'].edge_index = train_metapaths['Fasta', 'metapath_0', 'Fasta'].edge_index\n",
    "        if USE_HAS_HOST:\n",
    "            data['Fasta', 'metapath_1', 'Fasta'].edge_index = train_metapaths['Fasta', 'metapath_1', 'Fasta'].edge_index\n",
    "        return data\n",
    "    \n",
    "    metapaths = []\n",
    "    if ('Fasta', 'metapath_0', 'Fasta') not in data.edge_types:\n",
    "        metapaths.append([\n",
    "            ('Fasta', 'hasTaxRank', 'TargetTaxon'),\n",
    "            ('TargetTaxon', 'rev_hasTaxRank', 'Fasta'),\n",
    "        ])\n",
    "\n",
    "    if USE_HAS_HOST and ('Fasta', 'metapath_1', 'Fasta') not in data.edge_types:\n",
    "        metapaths.append([\n",
    "            ('Fasta', 'hasHostGenbank', 'HostTaxon'),\n",
    "            ('HostTaxon', 'rev_hasHostGenbank', 'Fasta'),\n",
    "        ])\n",
    "    \n",
    "    data = T.AddMetaPaths(metapaths=metapaths, weighted=True)(data)\n",
    "    return data\n",
    "\n",
    "def add_context_metapaths(data, train_metapaths):\n",
    "    metapaths = []\n",
    "    if ('Hit', 'metapath_0', 'Hit') not in data.edge_types:\n",
    "        metapaths.append([\n",
    "            ('Hit', 'rev_hasAffiliate', 'HitRegion'),\n",
    "            ('HitRegion', 'hasAffiliate', 'Hit'),\n",
    "        ])\n",
    "\n",
    "    if ('Hit', 'metapath_1', 'Hit') not in data.edge_types:\n",
    "        metapaths.append([\n",
    "            ('Hit', 'rev_hasAffiliate', 'HitRegion'),\n",
    "            ('HitRegion', 'hasDownstream', 'HitRegion'),\n",
    "            ('HitRegion', 'hasAffiliate', 'Hit'),\n",
    "        ])\n",
    "    \n",
    "    # if ('Hit', 'metapath_2', 'Hit') not in data.edge_types:\n",
    "    #     metapaths.append([\n",
    "    #         ('Hit', 'rev_hasMember', 'HitFamily'),\n",
    "    #         ('HitFamily', 'hasMember', 'Hit'),\n",
    "    #     ])\n",
    "\n",
    "    edge_types = data.edge_types\n",
    "    for mp in metapaths:\n",
    "        for edge in mp:\n",
    "            if edge not in edge_types:\n",
    "                print(f'Missing edge: {edge}')\n",
    "                # edge_types.append(edge)\n",
    "\n",
    "    data = T.AddMetaPaths(metapaths=metapaths, weighted=True)(data)\n",
    "\n",
    "    if train_metapaths:\n",
    "        data['Hit', 'metapath_0', 'Hit'].edge_index = torch.cat([\n",
    "            data['Hit', 'metapath_0', 'Hit'].edge_index,\n",
    "            train_metapaths['Hit', 'metapath_0', 'Hit'].edge_index\n",
    "        ], dim=-1)\n",
    "\n",
    "        data['Hit', 'metapath_1', 'Hit'].edge_index = torch.cat([\n",
    "            data['Hit', 'metapath_1', 'Hit'].edge_index,\n",
    "            train_metapaths['Hit', 'metapath_1', 'Hit'].edge_index\n",
    "        ], dim=-1)\n",
    "\n",
    "        # data['Hit', 'metapath_2', 'Hit'].edge_index = torch.cat([\n",
    "        #     data['Hit', 'metapath_2', 'Hit'].edge_index,\n",
    "        #     train_metapaths['Hit', 'metapath_2', 'Hit'].edge_index\n",
    "        # ], dim=-1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def add_reverse_edges(data):\n",
    "    data = T.ToUndirected(merge=False)(data)\n",
    "    # Remove \"reverse\" label (redundant when using link loader)\n",
    "    rev_edge = (TARGET_EDGE[2], f'rev_{TARGET_EDGE[1]}', TARGET_EDGE[0])\n",
    "    del data[rev_edge].edge_label\n",
    "\n",
    "    bug_edge = (TARGET_EDGE[0], f'rev_rev_{TARGET_EDGE[1]}', TARGET_EDGE[2])\n",
    "    if bug_edge in data.edge_types:\n",
    "        del data[bug_edge]\n",
    "    return data\n",
    "\n",
    "def get_inv_mappings(mappings):\n",
    "    inv_mappings = {}\n",
    "    for key, mapping in mappings.items():\n",
    "        inv_mappings[key] = {v: k for k, v in mapping.items()}\n",
    "    return inv_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: Modify graph features/attributes\n",
    "\n",
    "def add_pe_fts(data, train_data):\n",
    "    # handle negative sampling and homogenous graph conversion\n",
    "    cloned = data.clone()\n",
    "\n",
    "    if train_data:\n",
    "        for types in train_data.node_types + train_data.edge_types:\n",
    "            for key, value in train_data[types].items():\n",
    "                cloned[types][key] = value\n",
    "\n",
    "    if 'edge_label' in cloned['Fasta', 'hasTaxOrder', 'TargetTaxon']:\n",
    "        mask = cloned['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label == 1\n",
    "        cloned['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label = cloned['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label[mask]\n",
    "        cloned['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_index = torch.cat([\n",
    "            mask_select(cloned['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index[0], 0, mask),\n",
    "            mask_select(cloned['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index[1], 0, mask),\n",
    "        ], dim=-1)\n",
    "        del cloned['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index\n",
    "    homog_data = cloned.to_homogeneous()\n",
    "    # transform = T.AddRandomWalkPE(walk_length=3, attr_name='pe')\n",
    "    transform = T.AddLaplacianEigenvectorPE(k=16,  attr_name='pe')\n",
    "    homog_data = transform(homog_data)\n",
    "    start, stop = 0, 0\n",
    "    for i, node_type in enumerate(cloned.node_types):\n",
    "        stop = start + cloned[node_type].num_nodes - 1\n",
    "        assert homog_data.node_type[start] == i and homog_data.node_type[stop] == i\n",
    "        assert (stop - start) == cloned[node_type].num_nodes - 1\n",
    "        data[node_type].x = homog_data.pe[start:stop + 1]\n",
    "        start = stop + 1\n",
    "    return data\n",
    "\n",
    "def add_esm_fts(data, mappings):\n",
    "    with open(DATASET_DIR + 'eb.pkl', 'rb') as pickle_file:\n",
    "        embs = pickle.load(pickle_file)\n",
    "    xs = torch.zeros(data['Hit'].num_nodes, 320)\n",
    "    for emb_idx, emb_name in enumerate(embs['name']):\n",
    "        node_id = mappings['Hit'][emb_name]\n",
    "        node_idx = (data['Hit'].node_id == node_id).nonzero()[0]\n",
    "        xs[node_idx] = torch.tensor(embs['embedding'][emb_idx])\n",
    "    data['Hit'].x = torch.cat([data['Hit'].x, xs], dim=-1)\n",
    "    return data\n",
    "\n",
    "def add_llm_fts(data, mappings):\n",
    "    hit_family_embs = pd.read_csv(DATASET_DIR + 'pfam_llm.csv')\n",
    "\n",
    "    # xs = torch.zeros(data['HitFamily'].num_nodes, 384)\n",
    "    # for _, row in hit_family_embs.iterrows():\n",
    "    #     node_id = mappings['HitFamily'][row['accession']]\n",
    "    #     node_idx = (data['HitFamily'].node_id == node_id).nonzero()[0]\n",
    "    #     xs[node_idx] = torch.tensor(ast.literal_eval(row['embeddings']))\n",
    "    # data['HitFamily'].x = torch.cat([data['HitFamily'].x, xs], dim=-1)\n",
    "    # data['HitFamily'].x = xs\n",
    "\n",
    "    tax_order_embs = pd.read_csv(DATASET_DIR + 'taxon_order_llm.csv')\n",
    "    xs = torch.zeros(data['TargetTaxon'].num_nodes, 384)\n",
    "    for _, row in tax_order_embs.iterrows():\n",
    "        if row['taxId'] not in mappings['TargetTaxon']:\n",
    "            continue\n",
    "        node_id = mappings['TargetTaxon'][row['taxId']]\n",
    "        node_idx = (data['TargetTaxon'].node_id == node_id).nonzero()[0]\n",
    "        xs[node_idx] = torch.tensor(ast.literal_eval(row['embeddings']))\n",
    "    data['TargetTaxon'].x = torch.cat([data['TargetTaxon'].x, xs], dim=-1)\n",
    "    # data['TargetTaxon'].x = xs\n",
    "\n",
    "    return data\n",
    "\n",
    "def add_degree_fts(data):\n",
    "    node_types, edge_types = data.metadata()\n",
    "    for node_type in node_types:\n",
    "        for edge_type in edge_types:\n",
    "            if node_type not in edge_type:\n",
    "                continue\n",
    "            if 'rev' in edge_type[1]:\n",
    "                continue\n",
    "            if TARGET_EDGE[1] in edge_type[1]:\n",
    "                continue\n",
    "\n",
    "            idx = 0 if edge_type[0] == node_type else 1\n",
    "            x_degree = degree(\n",
    "                data[edge_type].edge_index[idx],\n",
    "                num_nodes=data[node_type].num_nodes,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "            if 'x' not in data[node_type]:\n",
    "                data[node_type].x = x_degree.view(-1, 1)\n",
    "            else:\n",
    "                data[node_type].x = torch.cat([data[node_type].x, x_degree.view(-1, 1)], dim=-1)\n",
    "        \n",
    "        data[node_type].x = torch.sum(data[node_type].x, dim=-1, keepdim=True)\n",
    "\n",
    "    return data\n",
    "      \n",
    "def get_all_taxon_ancestors(node_ids, data):\n",
    "    def get_taxon_ancestors(node_id, data):\n",
    "        target_parent_edges = data['TargetTaxon', 'hasParent', 'ContextTaxon'].edge_index\n",
    "        context_parent_edges = data['ContextTaxon', 'hasParent', 'ContextTaxon'].edge_index\n",
    "        ancestors = [node_id]\n",
    "        cur_idx = (target_parent_edges[0] == node_id).nonzero()\n",
    "        if target_parent_edges[1][cur_idx].numel() == 0:\n",
    "            return ancestors\n",
    "        cur_id = target_parent_edges[1][cur_idx].item()\n",
    "        while cur_id:\n",
    "            parent_edge = context_parent_edges[0] == cur_id\n",
    "            if parent_edge.sum() == 0:\n",
    "                break\n",
    "            cur_idx = parent_edge.nonzero()\n",
    "            cur_id = context_parent_edges[1][cur_idx].item()\n",
    "            ancestors.append(cur_id)\n",
    "        return ancestors  \n",
    "    \n",
    "    all_ancestors = []\n",
    "    for node_id in node_ids:\n",
    "        # all_ancestors.extend(get_taxon_ancestors(node_id, data))\n",
    "        all_ancestors.append(get_taxon_ancestors(node_id, data))\n",
    "\n",
    "    max_ancestors = max([len(ancestors) for ancestors in all_ancestors])\n",
    "    all_ancestors = [ancestors + [0] * (max_ancestors - len(ancestors)) for ancestors in all_ancestors]\n",
    "    all_ancestors = torch.tensor(all_ancestors, dtype=torch.long)\n",
    "    return all_ancestors.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: Split data, init data loaders\n",
    "\n",
    "def split_data(data):\n",
    "    num_test = TEST_FRACTION\n",
    "    num_val = VAL_FRACTION\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=num_val,\n",
    "        num_test=num_test,\n",
    "        # is_undirected=True,\n",
    "        # Of training edges, use 70% for message passing (edge_index)\n",
    "        # and 30% for supervision (edge_label_index)\n",
    "        # disjoint_train_ratio=0.3,\n",
    "        # Generate fixed negative edges in validation and test data\n",
    "        neg_sampling_ratio=NEGATIVE_SAMPLING_RATIO,\n",
    "        # Don't generate negative edges in training set, will be generated by LinkNeighborLoader.\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=TARGET_EDGE,\n",
    "        rev_edge_types=(TARGET_EDGE[2], f'rev_{TARGET_EDGE[1]}', TARGET_EDGE[0]),\n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    ref_data = data.clone()\n",
    "    return train_data, val_data, test_data, ref_data\n",
    "\n",
    "def enrich_split_data(data, mappings, metapaths=None, USE_METAPATHS=USE_METAPATHS):\n",
    "    cloned = data.clone()\n",
    "    if USE_HAS_HOST:\n",
    "        cloned = add_context_has_host(cloned, mappings)\n",
    "    \n",
    "    if COLLAPSE_TAXONOMY_GRAPH:\n",
    "        cloned = add_context_taxonomy_collapsed(cloned, mappings)\n",
    "    else: \n",
    "        cloned = add_context_taxonomy(cloned, mappings)\n",
    "    \n",
    "\n",
    "    cloned = add_reverse_edges(cloned)\n",
    "    cloned = add_degree_fts(cloned)\n",
    "\n",
    "    if USE_METAPATHS:\n",
    "        # cloned = USE_METAPATHS_to_data(cloned, metapaths)\n",
    "        cloned = add_context_metapaths(cloned, metapaths)\n",
    "        cloned = add_train_only_metapaths(cloned, metapaths)\n",
    "\n",
    "    if COLLAPSE_PANGENOME_GRAPH:\n",
    "        cloned = collapse_pangenome_graph(cloned)\n",
    "\n",
    "    # cloned = add_pe_fts(cloned, metapaths)\n",
    "\n",
    "    if USE_ESM_EMBEDDINGS:\n",
    "        cloned = add_esm_fts(cloned, mappings)\n",
    "\n",
    "    if USE_LLM_EMBEDDINGS:\n",
    "        cloned = add_llm_fts(cloned, mappings)\n",
    "    \n",
    "    return cloned\n",
    "\n",
    "def get_train_loader(split_data, batch_size=BATCH_SIZE):\n",
    "    edge_label_index = split_data[TARGET_EDGE].edge_label_index\n",
    "    edge_label = split_data[TARGET_EDGE].edge_label\n",
    "\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=split_data,\n",
    "        num_neighbors=NUM_NEIGHBORS, \n",
    "        neg_sampling=dict(mode='binary', amount=NEGATIVE_SAMPLING_RATIO),\n",
    "        edge_label=edge_label,\n",
    "        edge_label_index=(TARGET_EDGE, edge_label_index),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        # drop_last=True,\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "def get_val_loader(split_data, batch_size=BATCH_SIZE):\n",
    "    edge_label_index = split_data[TARGET_EDGE].edge_label_index\n",
    "    edge_label = split_data[TARGET_EDGE].edge_label\n",
    "\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=split_data,\n",
    "        num_neighbors=NUM_NEIGHBORS,\n",
    "        edge_label_index=(TARGET_EDGE, edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        # drop_last=True,\n",
    "    )\n",
    "    return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model (v1)\n",
    "\n",
    "# Recommender system sage example:\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/recommender_system.py\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, dropout=DROPOUT_RATE):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.conv_layer = SAGEConv # SAGEConv, GATConv, GCNConv,\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(self.conv_layer((-1, -1), hidden_channels))\n",
    "        self.batch_norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(self.conv_layer(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        self.convs.append(self.conv_layer(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv, batch_norm in zip(self.convs[:-1], self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            # x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def forward(self, z_dict, edge_label_index, target_edge=TARGET_EDGE):\n",
    "        x_src = z_dict[target_edge[0]][edge_label_index[0]]\n",
    "        x_dst = z_dict[target_edge[2]][edge_label_index[1]]\n",
    "        return (x_src * x_dst).sum(dim=-1)\n",
    "\n",
    "class HierarchicalEdgeDecoder(torch.nn.Module):\n",
    "    def forward(self, x_src, x_dst):\n",
    "        return (x_src * x_dst).sum(dim=-1)\n",
    "\n",
    "class ModelV1(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, data):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, out_channels, num_layers)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = HierarchicalEdgeDecoder() if USE_HIERARCHICAL_LOSS else EdgeDecoder()\n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index, target_edge=TARGET_EDGE):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        self.data.to(device)\n",
    "\n",
    "        if USE_HIERARCHICAL_LOSS:\n",
    "            taxon_ancestors = get_all_taxon_ancestors(edge_label_index[1], self.data)\n",
    "            x_src = z_dict['Fasta'][edge_label_index[0]]\n",
    "            x_dst = z_dict['TargetTaxon'][edge_label_index[1]]\n",
    "            outs = [self.decoder(x_src, x_dst)]\n",
    "\n",
    "            for i, ancestor_path in enumerate(taxon_ancestors):\n",
    "                for ancestor in ancestor_path:\n",
    "                    if ancestor in z_dict['ContextTaxon']:\n",
    "                        x_dst = z_dict['ContextTaxon'][ancestor]\n",
    "                        outs.append(self.decoder(x_src, x_dst))\n",
    "                    else:\n",
    "                        outs.append(torch.zeros_like(outs[-1]))\n",
    "\n",
    "            outs = torch.stack(outs, dim=-1)\n",
    "            return outs.T\n",
    "\n",
    "        return self.decoder(z_dict, edge_label_index, target_edge)\n",
    "\n",
    "def get_model_v1(data):\n",
    "    channels = 64 if USE_ESM_EMBEDDINGS or USE_LLM_EMBEDDINGS else 128\n",
    "\n",
    "    if USE_UNSUPERVISED_MODEL:\n",
    "        model = to_hetero(\n",
    "            GraphSAGE(\n",
    "                (-1, -1),\n",
    "                hidden_channels=128,\n",
    "                out_channels=64,\n",
    "                num_layers=3,\n",
    "                dropout=DROPOUT_RATE,\n",
    "                # act_first=True,\n",
    "            ),\n",
    "            data.metadata()).to(device)\n",
    "    else:\n",
    "        model = ModelV1(\n",
    "            hidden_channels=channels,\n",
    "            out_channels=channels,\n",
    "            num_layers=3,\n",
    "            data=data,\n",
    "        ).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-eval loop\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class HLN:\n",
    "    def __init__(self, alpha=1, beta=0.8, p_loss=3., a_incremental=1.3):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.p_loss = p_loss\n",
    "        self.a_incremental=a_incremental\n",
    "\n",
    "    def calculate_lloss(self, predictions, true_labels):\n",
    "        '''Calculates the layer loss.\n",
    "        '''\n",
    "        lloss = 0\n",
    "        alpha = self.alpha\n",
    "\n",
    "        for l in range(predictions.shape[0]):\n",
    "            lloss += F.binary_cross_entropy_with_logits(predictions[l], true_labels)\n",
    "            # lloss +=  nn.CrossEntropyLoss()(predictions[l], true_labels[l])\n",
    "            # alpha = alpha * self.a_incremental\n",
    "        return self.alpha * lloss\n",
    "\n",
    "    def calculate_dloss(self, predictions, true_labels):\n",
    "        '''Calculate the dependence loss.\n",
    "        '''\n",
    "        dloss = 0\n",
    "        for l in range(1, true_labels.shape[0]):\n",
    "            prev_lvl_pred = predictions[l-1]\n",
    "            current_lvl_pred = predictions[l]\n",
    "            D_l = torch.eq(current_lvl_pred, prev_lvl_pred).to(device)\n",
    "            l_prev = torch.where(prev_lvl_pred == true_labels[l-1], torch.FloatTensor([0]).to(device), torch.FloatTensor([1]).to(device))\n",
    "            l_curr = torch.where(current_lvl_pred == true_labels[l], torch.FloatTensor([0]).to(device), torch.FloatTensor([1]).to(device))\n",
    "\n",
    "            dloss += torch.sum(torch.pow(self.p_loss, D_l * l_prev) * torch.pow(self.p_loss, D_l * l_curr) - 1)\n",
    "\n",
    "        return self.beta * dloss\n",
    "\n",
    "HLN = HLN()\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model_args = (\n",
    "            batch.x_dict,\n",
    "            batch.edge_index_dict,\n",
    "        )\n",
    "        if not USE_UNSUPERVISED_MODEL:\n",
    "            model_args += (batch[TARGET_EDGE].edge_label_index,)\n",
    "        pred = model(*model_args)\n",
    "        target = batch[TARGET_EDGE].edge_label\n",
    "        if USE_UNSUPERVISED_MODEL:\n",
    "            pred_src = pred[TARGET_EDGE[0]][batch[TARGET_EDGE].edge_label_index[0]]\n",
    "            pred_dst = pred[TARGET_EDGE[2]][batch[TARGET_EDGE].edge_label_index[1]]\n",
    "            pred = (pred_src * pred_dst).sum(dim=-1)\n",
    "\n",
    "        if USE_HIERARCHICAL_LOSS:\n",
    "            dloss = HLN.calculate_dloss(pred, target)\n",
    "            lloss = HLN.calculate_lloss(pred, target)\n",
    "            loss = dloss + lloss\n",
    "        else: \n",
    "            loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "            # loss = F.mse_loss(pred, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) \n",
    "        total_examples += pred.numel()\n",
    "\n",
    "    return total_loss / total_examples\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        model_args = (\n",
    "            batch.x_dict,\n",
    "            batch.edge_index_dict,\n",
    "        )\n",
    "        if not USE_UNSUPERVISED_MODEL:\n",
    "            model_args += (batch[TARGET_EDGE].edge_label_index,)\n",
    "        pred = model(*model_args)\n",
    "        \n",
    "        if USE_UNSUPERVISED_MODEL: \n",
    "            pred_src = pred[TARGET_EDGE[0]][batch[TARGET_EDGE].edge_label_index[0]]\n",
    "            pred_dst = pred[TARGET_EDGE[2]][batch[TARGET_EDGE].edge_label_index[1]]\n",
    "            pred = (pred_src * pred_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "        if USE_HIERARCHICAL_LOSS:\n",
    "            pred = pred[0].sigmoid().view(-1).long().cpu()\n",
    "        else:\n",
    "            pred = pred.sigmoid().view(-1).long().cpu()\n",
    "\n",
    "        target = batch[TARGET_EDGE].edge_label.long().cpu()\n",
    "        \n",
    "        preds.append(pred)\n",
    "        targets.append(target)\n",
    "\n",
    "    pred = torch.cat(preds, dim=0).numpy()\n",
    "    target = torch.cat(targets, dim=0).numpy()\n",
    "\n",
    "    accuracy = accuracy_score(target, pred)\n",
    "    auc_pr = average_precision_score(target, pred)\n",
    "    # auc_roc = roc_auc_score(target, pred)\n",
    "    return accuracy, auc_pr\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_KNN(model, loader, device, k=3):\n",
    "    model.eval()\n",
    "\n",
    "    ref_data.to(device)\n",
    "    if not USE_UNSUPERVISED_MODEL:\n",
    "        dst_emb = model.encoder(ref_data.x_dict, ref_data.edge_index_dict)[TARGET_EDGE[2]]\n",
    "    else:\n",
    "        dst_emb = model(\n",
    "            ref_data.x_dict,\n",
    "            ref_data.edge_index_dict,\n",
    "        )[TARGET_EDGE[2]]\n",
    "\n",
    "    # Instantiate k-NN index based on maximum inner product search (MIPS):\n",
    "    mips = MIPSKNNIndex(dst_emb)\n",
    "\n",
    "    # Initialize metrics:\n",
    "    map_metric = LinkPredMAP(k=k).to(device)\n",
    "    precision_metric = LinkPredPrecision(k=k).to(device)\n",
    "    recall_metric = LinkPredRecall(k=k).to(device)\n",
    "\n",
    "    num_processed = 0\n",
    "    for batch in loader:  \n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute target embeddings:\n",
    "        if not USE_UNSUPERVISED_MODEL:\n",
    "            emb = model.encoder(batch.x_dict, batch.edge_index_dict)[TARGET_EDGE[0]]\n",
    "        else:\n",
    "            emb = model(\n",
    "                batch.x_dict,\n",
    "                batch.edge_index_dict,\n",
    "            )[TARGET_EDGE[2]]\n",
    "\n",
    "        num_processed += emb.size(0)\n",
    "\n",
    "        # Perform MIPS search:\n",
    "        _, pred_index_mat = mips.search(emb, k)\n",
    "\n",
    "        # Update retrieval metrics:\n",
    "        map_metric.update(pred_index_mat, batch[TARGET_EDGE].edge_label_index)\n",
    "        precision_metric.update(pred_index_mat, batch[TARGET_EDGE].edge_label_index)\n",
    "        recall_metric.update(pred_index_mat, batch[TARGET_EDGE].edge_label_index)\n",
    "\n",
    "    return (\n",
    "        float(map_metric.compute()),\n",
    "        float(precision_metric.compute()),\n",
    "        float(recall_metric.compute()),\n",
    "    )\n",
    "\n",
    "def update_stats(training_stats, epoch_stats):\n",
    "    if training_stats is None:\n",
    "        training_stats = {}\n",
    "        for key in epoch_stats.keys():\n",
    "            training_stats[key] = []\n",
    "    for key, val in epoch_stats.items():\n",
    "        training_stats[key].append(val)\n",
    "    return training_stats\n",
    "\n",
    "def train_and_eval_loop(model, train_loader, val_loader, test_loader):\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=L2_REG)\n",
    "    training_stats = None\n",
    "\n",
    "    for epoch in range(1, 50): \n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        train_acc, train_auc_pr = test(model, train_loader, device)\n",
    "        \n",
    "        test_acc, test_auc_pr = test(model, test_loader, device) if TEST_FRACTION > 0 else (0, 0)\n",
    "        val_acc, val_auc_pr = test(model, val_loader, device) if VAL_FRACTION > 0 else (0, 0)\n",
    "        test_lp_map, test_lp_precision, test_lp_recall = (\n",
    "            test_KNN(model, test_loader, device)\n",
    "            if TEST_FRACTION > 0 and not USE_UNSUPERVISED_MODEL\n",
    "            else (0, 0, 0)\n",
    "        )\n",
    "        \n",
    "        epoch_stats = {\n",
    "            'test_acc': test_acc,\n",
    "            'test_auc_pr': test_auc_pr,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_auc_pr': train_auc_pr,\n",
    "            'test_lp_map': test_lp_map,\n",
    "            'val_acc': val_acc,\n",
    "            'val_auc_pr': val_auc_pr,\n",
    "            'test_lp_precision': test_lp_precision,\n",
    "            'test_lp_recall': test_lp_recall,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "\n",
    "        training_stats = update_stats(training_stats, epoch_stats)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}\")\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Train AUC-PR: {train_auc_pr:.4f}\")\n",
    "            print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "            print(f\"Test AUC-PR: {test_auc_pr:.4f}\")\n",
    "            print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "            print(f\"Validation AUC-PR: {val_auc_pr:.4f}\")\n",
    "            print(\n",
    "                f\"Test LP MAP: {test_lp_map:.4f}, \"\n",
    "                f\"Test LP Precision: {test_lp_precision:.4f}, \"\n",
    "                f\"Test LP Recall: {test_lp_recall:.4f}\"\n",
    "            )\n",
    "            print()\n",
    "\n",
    "        if epoch > MIN_EPOCHS \\\n",
    "                and early_stopper.early_stop(test_acc):\n",
    "            break\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: Run exhaustive or KNN predictions\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_top_k_lp_predictions(\n",
    "    model,\n",
    "    data,\n",
    "    mappings,\n",
    "    k,\n",
    "):\n",
    "    clear_cuda_mem()\n",
    "    data.to(device)\n",
    "    \n",
    "    if 'edge_label_index' in data[TARGET_EDGE]:\n",
    "        edge_label_index = data[TARGET_EDGE].edge_label_index\n",
    "    else:\n",
    "        edge_label_index = data[TARGET_EDGE].edge_index\n",
    "\n",
    "\n",
    "    if not USE_UNSUPERVISED_MODEL:\n",
    "        dst_emb = model.encoder(data.x_dict, data.edge_index_dict)['TargetTaxon']\n",
    "    else:\n",
    "        dst_emb = model(data.x_dict, data.edge_index_dict)['TargetTaxon']\n",
    "\n",
    "    # Instantiate k-NN index based on maximum inner product search (MIPS):\n",
    "    mips = MIPSKNNIndex(dst_emb)\n",
    "\n",
    "    # Initialize metrics:\n",
    "    map_metric = LinkPredMAP(k=k).to(device)\n",
    "    precision_metric = LinkPredPrecision(k=k).to(device)\n",
    "    recall_metric = LinkPredRecall(k=k).to(device)\n",
    "\n",
    "\n",
    "    inv_fasta_mapping = {v: k for k, v in mappings['Fasta'].items()}\n",
    "    inv_taxon_mapping = {v: k for k, v in mappings['TargetTaxon'].items()}\n",
    "\n",
    "    data = data.to(device)\n",
    "    if not USE_UNSUPERVISED_MODEL:\n",
    "        emb = model.encoder(data.x_dict, data.edge_index_dict)['Fasta']\n",
    "    else:\n",
    "        emb = model(data.x_dict,data.edge_index_dict)['Fasta']\n",
    "\n",
    "    # for node_id in data['Fasta'].node_id:\n",
    "    #     if node_id not in edge_label_index[0]:\n",
    "    #         if node_id >= edge_label_index.shape[0]:\n",
    "    #             emb = torch.cat([emb, emb[node_id].unsqueeze(0)], dim=0)\n",
    "    #         else:\n",
    "    #             emb = torch.cat([emb[:node_id], emb[node_id+1:]], dim=0)\n",
    "        \n",
    "\n",
    "    # Perform MIPS search:\n",
    "    _, pred_index_mat = mips.search(emb, k)\n",
    "\n",
    "    print(pred_index_mat.shape)\n",
    "    print(edge_label_index.shape)\n",
    "\n",
    "    map_metric.update(pred_index_mat, edge_label_index)\n",
    "    precision_metric.update(pred_index_mat, edge_label_index)\n",
    "    recall_metric.update(pred_index_mat, edge_label_index)\n",
    "\n",
    "    pred_index_mat = pred_index_mat.cpu().numpy()\n",
    "    edge_label_index = edge_label_index.cpu().numpy()\n",
    "\n",
    "    taxon_nodes = get_virus_taxon_nodes()\n",
    "\n",
    "    rows = []\n",
    "    for i, row in enumerate(pred_index_mat):\n",
    "        try:\n",
    "            fasta_node_id = edge_label_index[0][i]\n",
    "            fasta_acc = inv_fasta_mapping[fasta_node_id]\n",
    "            taxon_node_id = edge_label_index[1][i]\n",
    "            tax_order = inv_taxon_mapping[taxon_node_id]\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        true_tax_order_name = taxon_nodes.loc[\n",
    "            taxon_nodes['taxId'] == tax_order\n",
    "        ]['taxOrder'].values[0]\n",
    "        \n",
    "        pred_tax_orders = []\n",
    "        for j in row:\n",
    "            pred_tax_orders.append(inv_taxon_mapping[j])\n",
    "        \n",
    "        rows.append({\n",
    "            'fasta': fasta_acc,\n",
    "            'pred_tax_orders': pred_tax_orders,\n",
    "            'true_tax_order': tax_order,\n",
    "            'true_tax_order_name': true_tax_order_name,\n",
    "            'correct': tax_order in pred_tax_orders\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    stats = {\n",
    "        'map': float(map_metric.compute()),\n",
    "        'precision': float(precision_metric.compute()),\n",
    "        'recall': float(recall_metric.compute()),\n",
    "    }\n",
    "    return (\n",
    "        df,\n",
    "        stats\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_exhaustive_lp_predictions(\n",
    "    model,\n",
    "    data,\n",
    "    mappings,\n",
    "    threshold=0.5,\n",
    "):\n",
    "    # https://github.com/pyg-team/pytorch_geometric/discussions/6792\n",
    "    file_path = DATASET_DIR + 'lp_pyg_preds.csv'\n",
    "\n",
    "    pangenome_df = pd.read_csv(DATASET_DIR + 'pangenome_nodes.csv')\n",
    "    fasta_df = pangenome_df.loc[\n",
    "        (pangenome_df['nodeLabels'] == 'Fasta') & \n",
    "        (pangenome_df['isSegmented'] == False)\n",
    "    ]\n",
    "\n",
    "    taxon_df = pd.read_csv(DATASET_DIR + 'taxon_nodes.csv')\n",
    "    taxon_df = taxon_df.loc[\n",
    "        (taxon_df['taxKingdom'] == 'Viruses') & \n",
    "        (taxon_df['rank'] == 'order')\n",
    "    ]\n",
    "\n",
    "    num_taxons = len(taxon_df)\n",
    "\n",
    "    # Nested loop over sotus to taxons\n",
    "    outs = []\n",
    "    for index, row in fasta_df[['appId', 'nodeId', 'order']].iterrows():\n",
    "        fasta_i = mappings['Fasta'][row['appId']]\n",
    "        row = torch.tensor([fasta_i] * num_taxons)\n",
    "        col = torch.arange(num_taxons)\n",
    "        edge_label_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        pred = model(\n",
    "            data.x_dict,\n",
    "            data.edge_index_dict,\n",
    "            edge_label_index\n",
    "        )\n",
    "        #pred = pred.sigmoid().view(-1).long().cpu()\n",
    "        pred = pred.clamp(min=0, max=1)\n",
    "        pred = (pred>threshold).float()\n",
    "\n",
    "        print(fasta_i)\n",
    "        print(pred)\n",
    "\n",
    "        # ground_truth = torch.zeros(num_taxons)\n",
    "        # ground_truth[mappings['TargetTaxon'][row['order']]] = 1\n",
    "        # print(ground_truth)\n",
    "\n",
    "        # for pred in preds:            \n",
    "        #     if pred > threshold:\n",
    "        #         outs.append({\n",
    "        #             'sourceNodeId': fasta_row['nodeId'],\n",
    "        #             'sourceAppId': fasta_app_id,\n",
    "        #             'targetNodeId': taxon_row['nodeId'],\n",
    "        #             'targetAppId': taxon_row['appId'],\n",
    "        #             'probability': pred,\n",
    "        #         })\n",
    "    # write preds to column\n",
    "    # df = pd.DataFrame(outs)\n",
    "    # df.to_csv(file_path, index=False)\n",
    "    return df\n",
    "\n",
    "# preds = run_exhaustive_lp_predictions(\n",
    "#     model,\n",
    "#     ref_data,\n",
    "#     mappings=mappings,\n",
    "#     threshold=0.2,\n",
    "# )\n",
    "# print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: Plot training stats and predictions\n",
    "\n",
    "def plot_training_stats(stats):\n",
    "    stats_df = pd.DataFrame(stats)\n",
    "    stats_df.set_index('epoch', inplace=True)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(3)\n",
    "\n",
    "    axs[0].plot(stats_df['train_acc'], label='train_acc')\n",
    "    axs[0].plot(stats_df['test_acc'], label='test_acc')\n",
    "    if VAL_FRACTION > 0:\n",
    "        axs[0].plot(stats_df['val_acc'], label='val_acc')\n",
    "    axs[0].set_title('Accuracy')\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(stats_df['train_auc_pr'], label='train_auc_pr')\n",
    "    axs[1].plot(stats_df['test_auc_pr'], label='test_auc_pr')\n",
    "    if VAL_FRACTION > 0:\n",
    "        axs[1].plot(stats_df['val_auc_pr'], label='val_auc_pr')\n",
    "    axs[1].set_ylim(0, 1)\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_title('AUC-PR')\n",
    "\n",
    "    axs[2].plot(stats_df['train_loss'], label='train_loss')\n",
    "    axs[2].set_title('Train loss')\n",
    "    axs[2].set_yscale('log')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # plot test_lp stats\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(3)\n",
    "\n",
    "    axs[0].plot(stats_df['test_lp_map'], label='test_lp_map')\n",
    "    axs[0].set_title('KNN MAP')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(stats_df['test_lp_precision'], label='test_lp_precision')\n",
    "    axs[1].set_title('KNN Precision')\n",
    "    axs[1].legend()\n",
    "\n",
    "    axs[2].plot(stats_df['test_lp_recall'], label='test_lp_recall')\n",
    "    axs[2].set_title('KNN Recall')\n",
    "    axs[2].legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions(results):\n",
    "    correct = results.loc[\n",
    "        results['correct'] == True\n",
    "    ]\n",
    "    incorrect = results.loc[\n",
    "        results['correct'] == False\n",
    "    ]\n",
    "    print(correct.shape)\n",
    "    print(incorrect.shape)\n",
    "    print(correct.shape[0] / (correct.shape[0] + incorrect.shape[0]))\n",
    "\n",
    "    correct_tax_order_counts = correct['true_tax_order_name'].value_counts()\n",
    "    incorrect_tax_order_counts = incorrect['true_tax_order_name'].value_counts()\n",
    "\n",
    "\n",
    "    # plot as grouped bar charts\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figwidth(8)\n",
    "    fig.set_figheight(3)\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    all_unique_tax_order_names = set(correct_tax_order_counts.index).union(set(incorrect_tax_order_counts.index))\n",
    "    index = np.arange(len(all_unique_tax_order_names))\n",
    "    index_names = list(all_unique_tax_order_names)\n",
    "    index_names = sorted(index_names, key=lambda x: correct_tax_order_counts.get(x, 0), reverse=True)\n",
    "\n",
    "    bar_width = 0.25\n",
    "    correct_tax_order_counts = correct_tax_order_counts.reindex(index_names, fill_value=0)\n",
    "    incorrect_tax_order_counts = incorrect_tax_order_counts.reindex(index_names, fill_value=0)\n",
    "\n",
    "    ax.bar(index, correct_tax_order_counts, bar_width, label='Correct predictions', color='green')\n",
    "    ax.bar(index + bar_width, incorrect_tax_order_counts, bar_width, label='Incorrect predictions', color='red')\n",
    "\n",
    "    ax.set_xlabel('Taxon ranks')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(correct_tax_order_counts.index, rotation=45, horizontalalignment='right')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "    # plot number of examples in each taxon rank\n",
    "    pangenome_nodes = get_filtered_pangenome_nodes()\n",
    "    fasta_nodes = pangenome_nodes.loc[\n",
    "        (pangenome_nodes['nodeLabels'] == 'Fasta') & \n",
    "        (pangenome_nodes['isSegmented'] == False)\n",
    "    ]\n",
    "    orders = fasta_nodes['order'].value_counts()\n",
    "    orders = orders.reindex(index_names, fill_value=0)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(index, orders, color='blue', label='Number of examples')\n",
    "    ax2.set_ylabel('Number of examples')\n",
    "    ax2.set_yscale('log')\n",
    "\n",
    "    # plot number of domains\n",
    "    domain_count = pd.read_csv(DATASET_DIR + 'fasta_domain_counts.csv')\n",
    "    domain_count = pd.merge(domain_count, fasta_nodes, left_on='fastaAccession', right_on='appId')\n",
    "    domain_count = domain_count.groupby('order')['domainCount'].sum()\n",
    "    domain_count = domain_count.reindex(index_names, fill_value=0)\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.plot(index, domain_count, color='orange', label='Number of domains')\n",
    "    ax3.set_ylabel('Number of domains')\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    ax3.spines['right'].set_visible(True)\n",
    "    ax3.yaxis.label.set_color('orange')\n",
    "    ax3.tick_params(axis='y', colors='orange')\n",
    "    \n",
    "    \n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    confusion_matrix = [[0, 0], [0, 0]]\n",
    "    confusion_matrix[0][0] = correct.shape[0]\n",
    "    confusion_matrix[1][1] = incorrect.shape[0]\n",
    "\n",
    "    # convert to numpy\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    confusion_matrix = pd.DataFrame(confusion_matrix, index=['True', 'False'], columns=['True', 'False'])\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(confusion_matrix, annot=True,  fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion matrix for {TARGET_RANK}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "\n",
    "def print_config_vars():\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Target edge: {TARGET_EDGE}\")\n",
    "    print(f\"Test fraction: {TEST_FRACTION}\")\n",
    "    print(f\"Val fraction: {VAL_FRACTION}\")\n",
    "    print(f\"Train fraction: {TRAIN_FRACTION}\")\n",
    "    print(f\"Negative sampling ratio: {NEGATIVE_SAMPLING_RATIO}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Num neighbors: {NUM_NEIGHBORS}\")\n",
    "    print(f\"Collapsing pangenome graph: {COLLAPSE_PANGENOME_GRAPH}\")\n",
    "    print(f\"Collapsing taxonomy graph: {COLLAPSE_TAXONOMY_GRAPH}\")\n",
    "    print(f\"Minimum epochs: {MIN_EPOCHS}\")\n",
    "    print(f\"Use hierarchical loss: {USE_HIERARCHICAL_LOSS}\")\n",
    "    print(f\"Use unsupervised model: {USE_UNSUPERVISED_MODEL}\")\n",
    "    print(f\"Use resampled data: {USE_RESAMPLED_DATASET}\")\n",
    "    print(f\"Use ESM embeddings: {USE_ESM_EMBEDDINGS}\")\n",
    "    print(f\"Use LLM embeddings: {USE_LLM_EMBEDDINGS}\")\n",
    "    print(f\"Include has host edge: {USE_HAS_HOST}\")\n",
    "    print(f\"Add metapaths: {USE_METAPATHS}\")\n",
    "    print()\n",
    "\n",
    "print_config_vars()\n",
    "\n",
    "data, mappings = create_pyg_graph()\n",
    "\n",
    "train_data, val_data, test_data, ref_data = split_data(data)\n",
    "\n",
    "train_data = enrich_split_data(train_data, mappings)\n",
    "val_data = enrich_split_data(val_data, mappings, train_data)\n",
    "test_data = enrich_split_data(test_data, mappings, train_data)\n",
    "ref_data = enrich_split_data(ref_data, mappings, USE_METAPATHS=False)\n",
    "\n",
    "train_loader = get_train_loader(train_data)\n",
    "val_loader = get_val_loader(val_data)\n",
    "test_loader = get_val_loader(test_data)\n",
    "\n",
    "inv_mappings = {}\n",
    "for key, mapping in mappings.items():\n",
    "    inv_mappings[key] = {v: k for k, v in mapping.items()}\n",
    "\n",
    "\n",
    "model = get_model_v1(ref_data)\n",
    "stats = train_and_eval_loop(model, train_loader, val_loader, test_loader)\n",
    "plot_training_stats(stats)\n",
    "\n",
    "results, stats = run_top_k_lp_predictions(model, ref_data, mappings, k=5)\n",
    "print(stats)\n",
    "\n",
    "plot_predictions(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers: Debugging and data validation\n",
    "\n",
    "print(train_data.keys())\n",
    "print(train_data.node_types)\n",
    "print(train_data.edge_types)\n",
    "\n",
    "print(test_data.keys())\n",
    "print(test_data.node_types)\n",
    "print(test_data.edge_types)\n",
    "\n",
    "# print(train_data['Hit'].num_nodes)\n",
    "# print(train_data['Hit'].x.shape)\n",
    "# print(train_data['Hit'].x)\n",
    "\n",
    "# IndexError: Found indices in 'edge_index' that are larger than 831 (got 8247626271654158368). Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 832) in your node feature matrix and try again.\n",
    "# convs_0__Hit1 = getattr(self.convs, \"0\").HitFamily__hasMember__Hit((x__HitFamily, x__Hit), edge_index__HitFamily__hasMember__Hit)\n",
    "\n",
    "# print(train_data[('HitFamily',  'hasMember', 'Hit')].edge_index[0].max())\n",
    "# print(train_data[('HitFamily',  'hasMember', 'Hit')].edge_index[1].max())\n",
    "# print(train_data[('Hit')].num_nodes)\n",
    "# print(train_data[('HitFamily')].num_nodes)\n",
    "\n",
    "\n",
    "# for batch in train_loader:\n",
    "#     print(batch['Hit'].num_nodes)\n",
    "#     print(batch['Hit'].num_nodes)\n",
    "#     print(batch['HitFamily'].num_nodes)\n",
    "#     print(batch[('HitFamily', 'hasMember', 'Hit')].edge_index)\n",
    "    # break\n",
    "\n",
    "\n",
    "def check_edge_index(data, mappings):\n",
    "    fasta_has_tax_order_edges = pd.read_csv(\n",
    "        f'{DATASET_DIR}fasta_has_tax_{TARGET_RANK}_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    edge_index = data[('Fasta', 'hasTaxRank', 'TargetTaxon')].edge_index\n",
    "        \n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src = edge_index[0, i].item()\n",
    "        dst = edge_index[1, i].item()\n",
    "        src_name = inv_mappings['Fasta'][src]\n",
    "        dst_name = inv_mappings['TargetTaxon'][dst]\n",
    "        try:\n",
    "            assert fasta_has_tax_order_edges.loc[\n",
    "                (fasta_has_tax_order_edges['sourceAppId'] == src_name) &\n",
    "                (fasta_has_tax_order_edges['targetAppId'] == dst_name)\n",
    "            ].shape[0] > 0\n",
    "        except:\n",
    "            print(f'Error: {src_name} -> {dst_name}')\n",
    "            print(f'Error: {src} -> {dst}')\n",
    "            print(\n",
    "                fasta_has_tax_order_edges.loc[\n",
    "                    (fasta_has_tax_order_edges['sourceAppId'] == src_name) \n",
    "                ]\n",
    "            )\n",
    "            break\n",
    "\n",
    "data_splits = [data, train_data, ref_data, val_data, test_data]\n",
    "for data in data_splits:\n",
    "    check_edge_index(data, mappings)\n",
    "\n",
    "# loaders = [train_loader, val_loader, test_loader]\n",
    "# for loader in loaders:\n",
    "#     for batch in loader:\n",
    "#         check_edge_index(batch, mappings)\n",
    "\n",
    "check_edge_index(data, mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-edge training\n",
    "\n",
    "\n",
    "model = get_model_v1(ref_data)\n",
    "\n",
    "all_stats = []\n",
    "# ('HitRegion', 'hasDownstream', 'HitRegion'),\n",
    "for edge_type in [('Fasta', 'hasTaxRank', 'TargetTaxon')]:\n",
    "    TARGET_EDGE = edge_type\n",
    "\n",
    "    train_data, val_data, test_data, ref_data = split_data(data)\n",
    "\n",
    "    train_data = enrich_split_data(train_data, mappings)\n",
    "    val_data = enrich_split_data(val_data, mappings, train_data)\n",
    "    test_data = enrich_split_data(test_data, mappings, train_data)\n",
    "    ref_data = enrich_split_data(ref_data, mappings, USE_METAPATHS=False)\n",
    "\n",
    "    train_loader = get_train_loader(train_data)\n",
    "    val_loader = get_val_loader(val_data)\n",
    "    test_loader = get_val_loader(test_data)\n",
    "\n",
    "    stats = train_and_eval_loop(model, train_loader, val_loader, test_loader)\n",
    "    all_stats.append(stats)\n",
    "\n",
    "# TARGET_EDGE = ('Fasta', 'hasTaxOrder', 'TargetTaxon')\n",
    "# stats = train_and_eval_loop(model, train_loader, val_loader, test_loader)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or load model weights\n",
    "\n",
    "def save_model(model):\n",
    "    model_name = 'model_v1'\n",
    "    weights_dir = f\"{MODEL_DIR}{model_name}\"\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    torch.save(model.state_dict(), weights_dir)\n",
    "\n",
    "\n",
    "def load_model(model_name='model_v1'):\n",
    "    model = get_model_v1(ref_data)\n",
    "    weights_dir = f\"{MODEL_DIR}{model_name}/\"\n",
    "    model.load_state_dict(torch.load(weights_dir))\n",
    "    return model\n",
    "\n",
    "\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP plot\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "train_data.to(device)\n",
    "\n",
    "if USE_UNSUPERVISED_MODEL:\n",
    "    dst_emb = model(train_data.x_dict, train_data.edge_index_dict)\n",
    "    dst_emb = dst_emb['Fasta']\n",
    "else:\n",
    "    dst_emb = model.encoder(train_data.x_dict, train_data.edge_index_dict)['Fasta']\n",
    "\n",
    "dst_emb = dst_emb.cpu()\n",
    "dst_emb = dst_emb.detach().numpy()\n",
    "\n",
    "# taxon_nodes = pd.read_csv(DATASET_DIR + 'pfam_llm.csv')\n",
    "# taxon_nodes = taxon_nodes.loc[\n",
    "#     (taxon_nodes['taxKingdom'] == 'Viruses') & \n",
    "#     (taxon_nodes['rank'] == 'order')\n",
    "# ]\n",
    "# dst_emb = taxon_nodes['embeddings'].values\n",
    "# dst_emb = [ast.literal_eval(i) for i in dst_emb]\n",
    "\n",
    "\n",
    "# dst_emb = test_data['HitFamily'].x.cpu().numpy()\n",
    "# dst_emb = train_data['Fasta'].x.cpu().numpy()\n",
    "\n",
    "umap_emb = reducer.fit_transform(dst_emb)\n",
    "\n",
    "plt.scatter(\n",
    "    umap_emb[:, 0],\n",
    "    umap_emb[:, 1],\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explainer\n",
    "\n",
    "from torch_geometric.explain import CaptumExplainer, Explainer\n",
    "\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/captum_explainer_hetero_link.py\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=CaptumExplainer('IntegratedGradients'),\n",
    "    explanation_type='model',\n",
    "    model_config=dict(\n",
    "        mode='regression',\n",
    "        task_level='edge',\n",
    "        return_type='raw',\n",
    "    ),\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    threshold_config=dict(\n",
    "        threshold_type='topk',\n",
    "        value=200,\n",
    "    ),\n",
    ")\n",
    "\n",
    "index = torch.tensor([2, 10])  # Explain edge labels with index 2 and 10.\n",
    "explanation = explainer(\n",
    "    data.x_dict,\n",
    "    data.edge_index_dict,\n",
    "    index=index,\n",
    "    edge_label_index=data['user', 'movie'].edge_label_index,\n",
    ")\n",
    "print(f'Generated explanations in {explanation.available_explanations}')\n",
    "\n",
    "path = 'feature_importance.png'\n",
    "explanation.visualize_feature_importance(top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot networkx (todo)\n",
    "\n",
    "from torch_geometric.utils import to_networkx, k_hop_subgraph\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Convert to networkx graph\n",
    "ref_data_homogenous = ref_data.to_homogeneous()\n",
    "subset, edge_index, mapping, edge_mask = k_hop_subgraph(10, 1, ref_data_homogenous.edge_index)\n",
    "print(ref_data_homogenous)\n",
    "print(subset)\n",
    "subgraph = Data(\n",
    "    x=ref_data_homogenous.x[subset],\n",
    "    node_id=subset,\n",
    "    edge_index=edge_index,\n",
    "    edge_label=ref_data_homogenous.edge_label[edge_mask],\n",
    "    edge_type=ref_data_homogenous.edge_type[edge_mask],\n",
    ")\n",
    "print(subgraph)\n",
    "subgraph = to_networkx(subgraph)\n",
    "\n",
    "# # Plot networkx graph\n",
    "nx.draw(subgraph, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "graph = to_networkx(test_data, to_undirected=False)\n",
    "\n",
    "# Define colors for nodes and edges\n",
    "node_type_colors = {\n",
    "    \"Station\": \"#4599C3\",\n",
    "    \"Lot\": \"#ED8546\",\n",
    "}\n",
    "\n",
    "node_colors = []\n",
    "labels = {}\n",
    "for node, attrs in graph.nodes(data=True):\n",
    "    node_type = attrs[\"type\"]\n",
    "    color =   \"#4599C3\" #node_type_colors[node_type]\n",
    "    node_colors.append(color)\n",
    "    if attrs[\"type\"] == \"Station\":\n",
    "        labels[node] = f\"S{node}\"\n",
    "    elif attrs[\"type\"] == \"Lot\":\n",
    "        labels[node] = f\"L{node}\"\n",
    "\n",
    "# Define colors for the edges\n",
    "edge_type_colors = {\n",
    "    (\"Lot\", \"SameSetup\", \"Station\"): \"#8B4D9E\",\n",
    "    (\"Station\", \"ShortSetup\", \"Lot\"): \"#DFB825\",\n",
    "    (\"Lot\", \"SameEnergySetup\", \"Station\"): \"#70B349\",\n",
    "    (\"Station\", \"ProcessNow\", \"Lot\"): \"#DB5C64\",\n",
    "}\n",
    "\n",
    "edge_colors = []\n",
    "for from_node, to_node, attrs in graph.edges(data=True):\n",
    "    edge_type = attrs[\"type\"]\n",
    "    color = \"#8B4D9E\" #edge_type_colors[edge_type]\n",
    "\n",
    "    graph.edges[from_node, to_node][\"color\"] = color\n",
    "    edge_colors.append(color)\n",
    "\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(graph, k=2)\n",
    "nx.draw_networkx(\n",
    "    graph,\n",
    "    pos=pos,\n",
    "    labels=labels,\n",
    "    with_labels=True,\n",
    "    node_color=node_colors,\n",
    "    edge_color=edge_colors,\n",
    "    node_size=600,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional topological link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_train_loader(train_data, batch_size=100)\n",
    "\n",
    "for batch in loader:\n",
    "    batch_homogenous = batch.to_homogeneous()\n",
    "    # print(batch_homogenous['edge_index'])\n",
    "    print(batch_homogenous['edge_label'])\n",
    "\n",
    "# print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_mapping = {v: k for k, v in mappings['TaxonOrder'].items()}\n",
    "print(len(taxon_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_train_loader(train_data, batch_size=2048)\n",
    "\n",
    "fasta_mapping = {v: k for k, v in mappings['Fasta'].items()}\n",
    "taxon_mapping = {v: k for k, v in mappings['TaxonOrder'].items()}\n",
    "fts = []\n",
    "\n",
    "for batch in loader:\n",
    "    batch_homogenous = batch.to_homogeneous()\n",
    "    G = to_networkx(batch_homogenous).to_undirected()\n",
    "\n",
    "    G_connected = G.copy()\n",
    "    isolated_nodes = [node for node in G.nodes() if G.degree(node) == 0]\n",
    "    [G_connected.remove_node(i_n) for i_n in isolated_nodes]\n",
    "\n",
    "    edge_index = batch_homogenous['edge_index'].tolist()\n",
    "    ebunch = list(zip(edge_index[0], edge_index[1]))\n",
    "    lp_iters = [\n",
    "        # ('jaccard_coefficient',\n",
    "        #  lambda: nx.jaccard_coefficient(G, ebunch)),\n",
    "        ('preferential_attachment',\n",
    "            lambda: nx.preferential_attachment(G, ebunch)),\n",
    "        # ('adamic_adar_index',\n",
    "        #  lambda: nx.adamic_adar_index(G_connected, ebunch)),\n",
    "        # ('resource_allocation_index',\n",
    "        #  lambda: nx.resource_allocation_index(G, ebunch)),\n",
    "        # ('common_neighbor_centrality',\n",
    "        #  lambda: nx.common_neighbor_centrality(G, ebunch)),\n",
    "    ]\n",
    "    fts_batch = []\n",
    "\n",
    "    for alg_name, get_iter in lp_iters:\n",
    "        iterable = get_iter()\n",
    "        print(alg_name)\n",
    "        for i, val in enumerate(iterable):\n",
    "            # Handle Adamic Adar division by 0 edge case\n",
    "            if i > len(batch_homogenous['edge_label']) - 1 or torch.isnan(\n",
    "                    batch_homogenous['edge_label'][i]):\n",
    "                print('herex')\n",
    "                if len(fts_batch) > i:\n",
    "                    fts_batch[i][alg_name] = 0\n",
    "                continue\n",
    "\n",
    "            if len(fts_batch) == 0:\n",
    "                print('reset')\n",
    "\n",
    "            if len(fts_batch) != i:\n",
    "                print('here4')\n",
    "                print(len(fts_batch), i)\n",
    "\n",
    "            if len(fts_batch) == i:\n",
    "                fts_batch.append({\n",
    "                    'sourceAppId': fasta_mapping[val[0]],\n",
    "                    'targetAppId': taxon_mapping[val[1]],\n",
    "                    'actual': batch_homogenous[\n",
    "                        'edge_label'][i].long().numpy(),\n",
    "                })\n",
    "            cur_row = fts_batch[i]\n",
    "            cur_row[alg_name] = val[2]\n",
    "    fts.extend(fts_batch)\n",
    "x = pd.DataFrame(fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_run_link_pred(loader, mappings):\n",
    "    fasta_mapping = {v: k for k, v in mappings['Fasta'].items()}\n",
    "    taxon_mapping = {v: k for k, v in mappings['TaxonOrder'].items()}\n",
    "    fts = []\n",
    "\n",
    "    for batch in loader:\n",
    "        print('here1')\n",
    "        batch_homogenous = batch.to_homogeneous()\n",
    "        G = to_networkx(batch_homogenous).to_undirected()\n",
    "        print('here2')\n",
    "\n",
    "        G_connected = G.copy()\n",
    "        isolated_nodes = [node for node in G.nodes() if G.degree(node) == 0]\n",
    "        [G_connected.remove_node(i_n) for i_n in isolated_nodes]\n",
    "\n",
    "        edge_index = batch_homogenous['edge_index'].tolist()\n",
    "        ebunch = list(zip(edge_index[0], edge_index[1]))\n",
    "        lp_iters = [\n",
    "            # ('jaccard_coefficient',\n",
    "            #  lambda: nx.jaccard_coefficient(G, ebunch)),\n",
    "            ('preferential_attachment',\n",
    "             lambda: nx.preferential_attachment(G, ebunch)),\n",
    "            # ('adamic_adar_index',\n",
    "            #  lambda: nx.adamic_adar_index(G_connected, ebunch)),\n",
    "            # ('resource_allocation_index',\n",
    "            #  lambda: nx.resource_allocation_index(G, ebunch)),\n",
    "            # ('common_neighbor_centrality',\n",
    "            #  lambda: nx.common_neighbor_centrality(G, ebunch)),\n",
    "        ]\n",
    "        print('here3')\n",
    "        fts_batch = []\n",
    "        for alg_name, get_iter in lp_iters:\n",
    "            iterable = get_iter()\n",
    "            \n",
    "            print(alg_name)\n",
    "            for i, val in enumerate(iterable):\n",
    "                # Handle Adamic Adar division by 0 edge case\n",
    "                if i > len(batch_homogenous['edge_label']) - 1 or torch.isnan(\n",
    "                        batch_homogenous['edge_label'][i]):\n",
    "                    if len(fts_batch) > i:\n",
    "                        fts_batch[i][alg_name] = 0\n",
    "                    continue\n",
    "\n",
    "                if len(fts_batch) == 0:\n",
    "                    print('reset')\n",
    "\n",
    "                if len(fts_batch) != i:\n",
    "                    print('here4')\n",
    "                    print(len(fts_batch), i)\n",
    "\n",
    "                if len(fts_batch) == i:\n",
    "                    fts_batch.append({\n",
    "                        'sourceAppId': fasta_mapping[val[0]],\n",
    "                        'targetAppId': taxon_mapping[val[1]],\n",
    "                        'actual': batch_homogenous[\n",
    "                            'edge_label'][i].long().numpy(),\n",
    "                    })\n",
    "                cur_row = fts_batch[i]\n",
    "                cur_row[alg_name] = val[2]\n",
    "        fts.extend(fts_batch)\n",
    "    return pd.DataFrame(fts)\n",
    "\n",
    "\n",
    "def nx_evaluate(ground_truths, preds):\n",
    "    accuracy = accuracy_score(ground_truths, preds)\n",
    "    precision = precision_score(ground_truths, preds)\n",
    "    recall = recall_score(ground_truths, preds)\n",
    "    f1 = f1_score(ground_truths, preds)\n",
    "    roc_auc = roc_auc_score(ground_truths, preds)\n",
    "    average_precision = average_precision_score(ground_truths, preds)\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\",\n",
    "               \"f1\", \"roc_auc\", \"average_precision\"]\n",
    "    values = [accuracy, precision, recall, f1, roc_auc, average_precision]\n",
    "    return pd.DataFrame(data={'metric': metrics, 'value': values})\n",
    "\n",
    "\n",
    "def nx_evaluate_link_pred(fts):\n",
    "    cols = [\n",
    "        # 'jaccard_coefficient',\n",
    "        'preferential_attachment',\n",
    "        # 'adamic_adar_index',\n",
    "        # 'resource_allocation_index',\n",
    "        # 'common_neighbor_centrality',\n",
    "    ]\n",
    "    stats = []\n",
    "    ground_truths = torch.tensor(fts['actual'].values.astype(int))\n",
    "    ground_truths = ground_truths.clamp(min=0, max=1)\n",
    "    for lp_alg in cols:\n",
    "        preds = torch.tensor(fts[lp_alg].values.astype(float))\n",
    "        # TODO: review if mid-point is best threshold for all algos\n",
    "        threshold = torch.div(torch.min(preds) + torch.max(preds), 2)\n",
    "        preds = (preds > threshold).clamp(min=0, max=1).long()\n",
    "        stats.append(\n",
    "            (lp_alg, evaluate(ground_truths, preds))\n",
    "        )\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, mappings = create_pyg_graph()\n",
    "# train_data, val_data, test_data = split_data(data)\n",
    "loader = get_train_loader(train_data)\n",
    "\n",
    "lp_fts = nx_run_link_pred(loader, mappings)\n",
    "stats = nx_evaluate_link_pred(lp_fts)\n",
    "\n",
    "# 1002684\n",
    "# 0\n",
    "# 0 513704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ljp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
