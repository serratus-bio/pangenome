{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pangenome taxonomy link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notebook config\n",
    "import sys\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "%load_ext dotenv\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "# Notebook imports\n",
    "import ast\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "import networkx as nx\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = './dataset/'\n",
    "EVALS_DIR = './evals/'\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j GDS Link prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_NAME_PREFIX = 'ml_projection_'\n",
    "\n",
    "NEO4J_URI = os.environ.get('NEO4J_URI')\n",
    "NEO4J_USER = os.environ.get('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD')\n",
    "\n",
    "TEST_FRACTION = 0.3\n",
    "TRAIN_FRACTION = 0.6\n",
    "MIN_EPOCHS = 10\n",
    "MAX_EPOCHS = 100\n",
    "VALIDATION_FOLDS = 5  # used to tune hyperparameters and determine best model\n",
    "NEGATIVE_SAMPLING_RATIO = 1 # (q - r) / r, where q = n(n-1)/2, n = number of nodes, r = actual number of edges\n",
    "NEGATIVE_CLASS_WEIGHT = 1# / NEGATIVE_SAMPLING_RATIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gds_client():\n",
    "    return GraphDataScience(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "def get_projection_name(version=1):\n",
    "    return PROJECTION_NAME_PREFIX + str(version)\n",
    "\n",
    "\n",
    "def get_projection(gds, projection_name, for_predictions=False):\n",
    "    if gds.graph.exists(projection_name)['exists']:\n",
    "        return gds.graph.get(projection_name)\n",
    "    \n",
    "    pangenome_nodes = pd.read_csv(DATASET_DIR + 'pangenome_nodes.csv')\n",
    "    taxon_nodes = pd.read_csv(DATASET_DIR + 'taxon_nodes.csv')\n",
    "\n",
    "    # prune taxonomy tree to only include viruses\n",
    "    taxon_nodes = taxon_nodes.loc[\n",
    "        taxon_nodes['taxKingdom'] == 'Viruses'\n",
    "    ]\n",
    "    taxon_nodes['nodeLabels'] = [\n",
    "        ['Taxon', 'TargetRank'] \n",
    "        if x == 'order' \n",
    "        else 'Taxon' \n",
    "        for x in taxon_nodes['rank']\n",
    "    ]\n",
    "\n",
    "    # assert there's no collision of nodeIds from diffferent DB instances\n",
    "    assert len(set(pangenome_nodes['nodeId'].values).intersection(set(taxon_nodes['nodeId'].values))) == 0\n",
    "\n",
    "    nodes = pd.concat([pangenome_nodes, taxon_nodes])\n",
    "    nodes['labels'] = nodes['nodeLabels']\n",
    "    # prune segmeneted fasta nodes\n",
    "    nodes = nodes.loc[\n",
    "        (nodes['labels'] != 'Fasta') |\n",
    "        (nodes['isSegmented'] == False)\n",
    "    ]\n",
    "    nodes = nodes[[\n",
    "        'nodeId',\n",
    "        'labels',\n",
    "        'degree',\n",
    "    ]]\n",
    "\n",
    "    pangenome_rels = pd.read_csv(DATASET_DIR + 'pangenome_edges.csv')\n",
    "    taxonomy_rels = pd.read_csv(DATASET_DIR + 'taxon_has_parent_edges.csv')\n",
    "    taxonomy_rels = taxonomy_rels.loc[\n",
    "        taxonomy_rels['sourceNodeId'].isin(taxon_nodes['nodeId'].values) &\n",
    "        taxonomy_rels['targetNodeId'].isin(taxon_nodes['nodeId'].values)\n",
    "    ]\n",
    "\n",
    "    pangenome_taxonomy_rels = pd.read_csv(DATASET_DIR + 'fasta_has_tax_order_edges.csv')\n",
    "    relationships = pd.concat([\n",
    "        pangenome_rels,\n",
    "        taxonomy_rels,\n",
    "        pangenome_taxonomy_rels,\n",
    "    ])\n",
    "\n",
    "    relationships = relationships[[\n",
    "        'sourceNodeId',\n",
    "        'targetNodeId',\n",
    "        'relationshipType',\n",
    "        'weight'\n",
    "    ]]\n",
    "\n",
    "    undirected_relationship_types = relationships['relationshipType'].unique().tolist()\n",
    "    if not for_predictions:\n",
    "        # undirected_relationship_types.remove('hasTaxOrder')\n",
    "        undirected_relationship_types.remove('hasHit')\n",
    "        undirected_relationship_types.remove('hasRegion')\n",
    "        # undirected_relationship_types.remove('hasAffiliate')\n",
    "        undirected_relationship_types.remove('hasDownstream')\n",
    "        # undirected_relationship_types.remove('hasMember')\n",
    "        \n",
    "    projection = gds.graph.construct(\n",
    "        graph_name=projection_name,\n",
    "        nodes=nodes,\n",
    "        relationships=relationships,\n",
    "        concurrency=4,\n",
    "        undirected_relationship_types=undirected_relationship_types,\n",
    "    )\n",
    "    return projection\n",
    "\n",
    "def delete_projection(gds, projection_name):\n",
    "    if gds.graph.exists(projection_name)['exists']:\n",
    "        gds.graph.drop(gds.graph.get(projection_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "projection_name = get_projection_name(version)\n",
    "gds = get_gds_client()\n",
    "delete_projection(gds, projection_name)\n",
    "projection = get_projection(gds, projection_name)\n",
    "print(projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSage test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GraphSage cannot be trained in the GDS pipeline, and it doesn't seem to be possible to get the test split from the gds pipeline.\n",
    "- To avoid info leakeage, we'd need to create a seperate dataset to train graphsage.\n",
    "- We can also consider resampling the dataset to balance each of the taxonomy orders with varying number of hits to produce better embeddings.\n",
    "- Running graphSage on the current neo4j instance causes it to crash, will try this pipeline in pytorch on a high-memory instance later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = gds.alpha.ml.splitRelationships.mutate(\n",
    "    G=gds.graph.get(projection_name),\n",
    "    sourceNodeLabels=['Fasta'],\n",
    "    targetNodeLabels=['TargetRank'],\n",
    "    relationshipTypes=['hasTaxOrder'],\n",
    "    holdoutFraction=TEST_FRACTION,\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    holdoutRelationshipType='hasTaxOrder_TEST',\n",
    "    remainingRelationshipType='hasTaxOrder_TRAIN',\n",
    "    negativeSamplingRatio=NEGATIVE_SAMPLING_RATIO,\n",
    "    relationshipWeightProperty='weight',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "gds.fastRP.mutate(\n",
    "    G=gds.graph.get(projection_name),\n",
    "    mutateProperty='fastRP',\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    # relationshipWeightProperty='weight',\n",
    "    # featureProperties=[\"degree\"],\n",
    "    nodeLabels=[\n",
    "        'Taxon',\n",
    "        'Fasta',\n",
    "        'Hit',\n",
    "        'HitFamily',\n",
    "        'HitRegion',\n",
    "    ],\n",
    "    relationshipTypes=[\n",
    "        # \"*\",\n",
    "        'hasTaxOrder_TRAIN',\n",
    "        'HAS_PARENT',\n",
    "        'hasHit',\n",
    "        'hasRegion',\n",
    "        'hasAffiliate',\n",
    "        'hasMember',\n",
    "        'hasDownstream',\n",
    "    ],\n",
    "    embeddingDimension=128,\n",
    "    normalizationStrength=-0.5,\n",
    "    # nodeSelfInfluence=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gds.beta.model.exists('graphSage')['exists']:\n",
    "    gds.beta.model.drop(gds.model.get('graphSage'))\n",
    "\n",
    "gds.beta.graphSage.train(\n",
    "    G=gds.graph.get(projection_name),\n",
    "    modelName='graphSage',\n",
    "    featureProperties=['fastRP'],\n",
    "    nodeLabels=[\n",
    "        'Taxon',\n",
    "        'Fasta',\n",
    "        'Hit',\n",
    "        'HitFamily',\n",
    "        'HitRegion',\n",
    "    ],\n",
    "    relationshipTypes=[\n",
    "        # '*'\n",
    "        'hasTaxOrder_TEST',\n",
    "        'HAS_PARENT',\n",
    "        'hasHit',\n",
    "        'hasRegion',\n",
    "        'hasAffiliate',\n",
    "        'hasMember',\n",
    "        'hasDownstream',\n",
    "    ],\n",
    "    embeddingDimension=256,\n",
    "    epochs=10,\n",
    "    searchDepth=10,\n",
    "    # relationshipWeightProperty='weight',\n",
    "    # featureProperties=['fastRP', 'degree'],\n",
    "    # featureProperties=['degree'],\n",
    "    # featureProperties=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_sage_embeddings = gds.beta.graphSage.stream(\n",
    "#     G=gds.graph.get(projection_name),\n",
    "#     modelName='graphSage',\n",
    "#     nodeLabels=[\n",
    "#         'Taxon',\n",
    "#         'Fasta',\n",
    "#         'Hit',\n",
    "#         'HitFamily',\n",
    "#         'HitRegion',\n",
    "#     ],\n",
    "#     relationshipTypes=['*'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-eval pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "\n",
    "pipeline_name = 'taxon_lp_pipeline'\n",
    "model_encoder = 'fastRP'\n",
    "\n",
    "if gds.beta.pipeline.exists(pipeline_name)['exists']:\n",
    "    gds.beta.pipeline.drop(gds.pipeline.get(pipeline_name))\n",
    "\n",
    "pipeline, _ = gds.beta.pipeline.linkPrediction.create(pipeline_name)\n",
    "\n",
    "G = gds.graph.get(projection_name)\n",
    "\n",
    "pipeline.addNodeProperty(\n",
    "    procedure_name=model_encoder,\n",
    "    mutateProperty=model_encoder,\n",
    "    embeddingDimension=384, # 384\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    # featureProperties=[\"features\"],\n",
    "    relationshipWeightProperty='weight',\n",
    "    normalizationStrength=-0.5,\n",
    "    # nodeSelfInfluence=2,\n",
    "    contextRelationshipTypes=[\n",
    "        \"HAS_PARENT\",\n",
    "        \"hasHit\",\n",
    "        \"hasRegion\",\n",
    "        \"hasAffiliate\",\n",
    "        \"hasMember\",\n",
    "        \"hasDownstream\",\n",
    "    ],\n",
    "    contextNodeLabels=[\n",
    "        \"Taxon\",\n",
    "        \"Fasta\",\n",
    "        \"Hit\",\n",
    "        \"HitRegion\",\n",
    "        \"HitFamily\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# L2, HADAMARD, COSINE, SAME_CATEGORY\n",
    "pipeline.addFeature(\"hadamard\", nodeProperties=[model_encoder])\n",
    "\n",
    "# pipeline.addNodeProperty(\n",
    "#     procedure_name=\"beta.graphSage\",\n",
    "#     mutateProperty=\"graphSage\",\n",
    "#     modelName=\"graphSage\",\n",
    "#     # featureProperties=[\"fastRP\"],\n",
    "#     contextRelationshipTypes=[\n",
    "#         \"HAS_PARENT\",\n",
    "#         \"hasHit\",\n",
    "#         \"hasRegion\",\n",
    "#         \"hasAffiliate\",\n",
    "#         \"hasMember\",\n",
    "#         \"hasDownstream\",\n",
    "#     ],\n",
    "#     contextNodeLabels=[\n",
    "#         \"Taxon\",\n",
    "#         \"Fasta\",\n",
    "#         \"Hit\",\n",
    "#         \"HitRegion\",\n",
    "#         \"HitFamily\",\n",
    "#     ],\n",
    "#     # relationshipWeightProperty=\"weight\",\n",
    "# )\n",
    "\n",
    "# pipeline.addFeature(\"hadamard\", nodeProperties=[\"graphSage\"])\n",
    "\n",
    "split = pipeline.configureSplit(\n",
    "    testFraction=TEST_FRACTION,\n",
    "    validationFolds=VALIDATION_FOLDS,\n",
    "    negativeSamplingRatio=NEGATIVE_SAMPLING_RATIO,\n",
    ")\n",
    "\n",
    "pipeline.addLogisticRegression(penalty=(0.1, 2))\n",
    "# pipeline.addLogisticRegression(maxEpochs=200, penalty=(0.0, 2))\n",
    "pipeline.addRandomForest(maxDepth=(2, 50))\n",
    "# pipeline.addRandomForest(maxDepth=(2, 100))\n",
    "\n",
    "# Last layer should match number of classes being predicted\n",
    "# pipeline.addMLP(hiddenLayerSizes=[16, 16, 4], penalty=1, patience=2)\n",
    "# pipeline.addMLP(hiddenLayerSizes=[128, 64, 2], maxEpochs=100, patience=10)\n",
    "# [128, 256, 128, 2]\n",
    "# [256, 128, 2] *\n",
    "# [256, 384, 256, 128, 2]\n",
    "pipeline.addMLP(\n",
    "    # hiddenLayerSizes=[256, 128, 2], #[384, 128, 2],\n",
    "    hiddenLayerSizes=[384, 128, 2],\n",
    "    maxEpochs=100,\n",
    "    patience=10,\n",
    "    focusWeight=0.5,\n",
    "    # focusWeight={'range': [0.0, 0.5]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the train and eval pipeline\n",
    "\n",
    "if gds.beta.model.exists(model_encoder)['exists']:\n",
    "    gds.beta.model.drop(gds.model.get(model_encoder))\n",
    "\n",
    "model, evals = pipeline.train(\n",
    "    pipeline=pipeline,\n",
    "    G=gds.graph.get(projection_name),\n",
    "    modelName=model_encoder,\n",
    "    randomSeed=RANDOM_SEED,\n",
    "    targetRelationshipType='hasTaxOrder',\n",
    "    sourceNodeLabel='Fasta',\n",
    "    targetNodeLabel='TargetRank',\n",
    "    negativeClassWeight=NEGATIVE_CLASS_WEIGHT,\n",
    "    metrics=[ \n",
    "        'AUCPR'\n",
    "    ],\n",
    ")\n",
    "\n",
    "evals.to_csv(EVALS_DIR + 'lp_gds_evals.csv')\n",
    "display(evals['modelInfo']['bestParameters'])\n",
    "\n",
    "print('-----------------')\n",
    "\n",
    "print('Best model metrics (general):')\n",
    "print('AUCPR:')\n",
    "display(evals['modelInfo']['metrics']['AUCPR'])\n",
    "\n",
    "# {'test': 0.8079789288270317,\n",
    "#  'validation': {'min': 0.7552969944662513,\n",
    "#   'max': 0.7978044070290363,\n",
    "#   'avg': 0.7801199361717549},\n",
    "#  'outerTrain': 0.99993761244742,\n",
    "#  'train': {'min': 0.9999357007540141,\n",
    "#   'max': 0.9999999999999999,\n",
    "#   'avg': 0.999974082447445}}\n",
    "\n",
    "# {'test': 0.7165068713762617,\n",
    "#  'validation': {'min': 0.6341140732932966,\n",
    "#   'max': 0.7483135387115906,\n",
    "#   'avg': 0.6846875847262417},\n",
    "#  'outerTrain': 0.9999455163442132,\n",
    "#  'train': {'min': 0.9999510657293991,\n",
    "#   'max': 0.9999836847323984,\n",
    "#   'avg': 0.9999713434218448}}\n",
    "\n",
    "# {'test': 0.9886814718381982,\n",
    "#  'validation': {'min': 0.985947330770067,\n",
    "#   'max': 0.9909918266054518,\n",
    "#   'avg': 0.9887788612948111},\n",
    "#  'outerTrain': 0.9989832669230688,\n",
    "#  'train': {'min': 0.9989039358314438,\n",
    "#   'max': 0.998947612674645,\n",
    "#   'avg': 0.9989210279491374}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "\n",
    "projection_name = get_projection_name(version)\n",
    "gds = get_gds_client()\n",
    "delete_projection(gds, projection_name)\n",
    "projection = get_projection(gds, projection_name, for_predictions=True)\n",
    "print(projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map nodeIds to appIds\n",
    "\n",
    "def enrich_app_data(df):\n",
    "    pangenome_nodes = pd.read_csv(DATASET_DIR + 'pangenome_nodes.csv')\n",
    "    fasta_nodes = pangenome_nodes.loc[\n",
    "        pangenome_nodes['nodeLabels'] == 'Fasta'\n",
    "    ]\n",
    "    fasta_nodes = fasta_nodes[[\n",
    "        'nodeId',\n",
    "        'appId',\n",
    "        'order',\n",
    "    ]]\n",
    "    taxon_nodes = pd.read_csv(DATASET_DIR + 'taxon_nodes.csv')\n",
    "    taxon_nodes = taxon_nodes.loc[\n",
    "        taxon_nodes['taxKingdom'] == 'Viruses'\n",
    "    ]\n",
    "    taxon_nodes = taxon_nodes[[\n",
    "        'nodeId',\n",
    "        'appId',\n",
    "        'taxOrder',\n",
    "        'rank',\n",
    "    ]]\n",
    "\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        fasta_nodes,\n",
    "        how='left',\n",
    "        left_on='sourceNodeId',\n",
    "        right_on='nodeId',\n",
    "    )\n",
    "    df.drop('nodeId', axis=1, inplace=True)\n",
    "    df.rename(columns={'appId': 'sourceAppId', 'order': 'expectedOrder'}, inplace=True)\n",
    "\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        taxon_nodes,\n",
    "        how='left',\n",
    "        left_on='targetNodeId',\n",
    "        right_on='nodeId',\n",
    "    )\n",
    "    df.drop('nodeId', axis=1, inplace=True)\n",
    "    df.rename(columns={'appId': 'targetAppId', 'taxOrder': 'predictedOrder', 'rank': 'predictedRank'}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def format_predictions(predictions):\n",
    "    predictions = predictions.sort_values(['node1', 'probability'], ascending=[True, False])\n",
    "    predictions['sourceNodeId'] = predictions['node1']\n",
    "    predictions.drop('node1', axis=1, inplace=True)\n",
    "    predictions['targetNodeId'] = predictions['node2']\n",
    "    predictions.drop('node2', axis=1, inplace=True)\n",
    "\n",
    "    merged = enrich_app_data(predictions)\n",
    "    # merged.dropna(inplace=True)\n",
    "\n",
    "    if merged['sourceAppId'].isna().sum() > 0:\n",
    "        non_null = merged.loc[\n",
    "            ~merged['sourceAppId'].isna()\n",
    "        ]\n",
    "        reversed_direction = merged.loc[\n",
    "            merged['sourceAppId'].isna()\n",
    "        ]\n",
    "        reversed_direction[['sourceNodeId','targetNodeId']] = reversed_direction[['targetNodeId','sourceNodeId']]\n",
    "        reversed_direction.drop([\n",
    "                'sourceAppId',\n",
    "                'targetAppId',\n",
    "                'expectedOrder',\n",
    "                'predictedOrder',\n",
    "                'predictedRank'\n",
    "            ], axis=1, inplace=True)\n",
    "        reversed_direction = enrich_app_data(reversed_direction)\n",
    "        merged = pd.concat([non_null, reversed_direction])\n",
    "\n",
    "    merged['correct'] = merged['expectedOrder'] == merged['predictedOrder']\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_exhaustive_predictions(model):\n",
    "    # Stream approximate predictions\n",
    "    predictions = model.predict_stream(\n",
    "        G=gds.graph.get(projection_name),\n",
    "        # topK=1,\n",
    "        topN=6318,\n",
    "        # randomSeed=RANDOM_SEED,\n",
    "        sourceNodeLabel='Fasta',\n",
    "        targetNodeLabel='TargetRank',\n",
    "        relationshipTypes=[\n",
    "            # \"hasTaxOrder\",\n",
    "            \"HAS_PARENT\",\n",
    "            \"hasHit\",\n",
    "            \"hasRegion\",\n",
    "            \"hasAffiliate\",\n",
    "            \"hasMember\",\n",
    "            \"hasDownstream\",\n",
    "        ],\n",
    "        sampleRate=1, # 1 = exhaustive\n",
    "        # threshold=0.5,\n",
    "        # randomJoins=2,\n",
    "        # maxIterations=3,\n",
    "        # deltaThreshold=0.01,\n",
    "    )\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def stream_approximate_predictions(model):\n",
    "    predictions = model.predict_stream(\n",
    "        G=gds.graph.get(projection_name),\n",
    "        topK=1,\n",
    "        # randomSeed=RANDOM_SEED,\n",
    "        sourceNodeLabel='Fasta',\n",
    "        targetNodeLabel='TargetRank',\n",
    "        relationshipTypes=[\n",
    "            # \"hasTaxOrder\",\n",
    "            \"HAS_PARENT\",\n",
    "            \"hasHit\",\n",
    "            \"hasRegion\",\n",
    "            \"hasAffiliate\",\n",
    "            \"hasMember\",\n",
    "            \"hasDownstream\",\n",
    "        ],\n",
    "        sampleRate=0.1, # 1 = exhaustive\n",
    "        # threshold=0.5,\n",
    "        # randomJoins=2,\n",
    "        # maxIterations=3,\n",
    "        # deltaThreshold=0.01,\n",
    "    )\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gds.model.get('fastRP')\n",
    "# preds_raw = stream_exhaustive_predictions(model)\n",
    "preds_raw = stream_approximate_predictions(model)\n",
    "preds = format_predictions(preds_raw)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv(DATASET_DIR + 'lp_gds_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds['sourceAppId'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds['probability'].describe())\n",
    "print(preds_raw['node1'].nunique())\n",
    "print(preds['sourceNodeId'].nunique())\n",
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds['correct'].value_counts())\n",
    "# correct\n",
    "# False    61282\n",
    "# True      1038\n",
    "\n",
    "# group by source and check if any targets are correct\n",
    "grouped = preds.groupby('sourceAppId')\n",
    "grouped_correct = grouped['correct'].any()\n",
    "print(grouped_correct.value_counts())\n",
    "\n",
    "# correct\n",
    "# False    5372\n",
    "# True      788\n",
    "\n",
    "# False    5264\n",
    "# True      968\n",
    "\n",
    "# False    5194\n",
    "# True     1038\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.graph.drop(get_projection_name())\n",
    "gds.beta.model.drop(gds.model.get(model_encoder))\n",
    "gds.beta.pipeline.drop(gds.pipeline.get(pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install GPU enabled pytorch, pyg in conda env** \n",
    "\n",
    "``` \n",
    "conda remove -n ljp --all\n",
    "pip cache purge\n",
    "\n",
    "conda create --name ljp python=3.8 ipython\n",
    "conda activate ljp\n",
    "conda install -y ipykernel\n",
    "\n",
    "conda install -y cuda -c nvidia\n",
    "conda install -y cudatoolkit\n",
    "conda install -y cudnn\n",
    "\n",
    "nvcc --version\n",
    "\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "python -c \"import torch; print(torch.__version__)\" \n",
    "#2.3.0+cu121\n",
    "\n",
    "pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
    "\n",
    "pip install -r requirements.txt\n",
    "pip install faiss-gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric import seed_everything\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.utils import to_networkx, degree\n",
    "from torch_geometric.nn import SAGEConv, GATConv, to_hetero, MIPSKNNIndex, BatchNorm\n",
    "import torch_geometric.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.metrics import (\n",
    "    LinkPredMAP,\n",
    "    LinkPredPrecision,\n",
    "    LinkPredRecall,\n",
    ")\n",
    "\n",
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './models/'\n",
    "\n",
    "NEGATIVE_SAMPLING_RATIO = 1\n",
    "TEST_FRACTION = 0.2\n",
    "VAL_FRACTION = 0\n",
    "TRAIN_FRACTION = 1 - TEST_FRACTION - VAL_FRACTION\n",
    "MAX_EPOCHS = 100\n",
    "MIN_EPOCHS = 5\n",
    "BATCH_SIZE = 1024 # 2048 # 4096\n",
    "NUM_NEIGHBORS = [-1, -1, -1, -1]\n",
    "\n",
    "ADD_METAPATHS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityEncoder(object):\n",
    "    # The 'IdentityEncoder' takes the raw column values and converts them to\n",
    "    # PyTorch tensors.\n",
    "    def __init__(self, dtype=None, is_list=False, is_tensor=False):\n",
    "        self.dtype = dtype\n",
    "        self.is_list = is_list\n",
    "        self.is_tensor = is_tensor\n",
    "\n",
    "    def __call__(self, df):\n",
    "        if self.is_tensor:\n",
    "            if self.is_list:\n",
    "                return torch.stack([torch.tensor([el]) for el in df.values])\n",
    "            return torch.from_numpy(df.values).to(self.dtype)\n",
    "        else:\n",
    "            return df\n",
    "\n",
    "\n",
    "class ListEncoder(object):\n",
    "    def __init__(self, sep=',', is_tensor=False):\n",
    "        self.sep = sep\n",
    "        self.is_tensor = is_tensor\n",
    "\n",
    "    def __call__(self, df):\n",
    "        eval_df = df.apply(\n",
    "            lambda x: [val for val in ast.literal_eval(x)])\n",
    "        return torch.stack([torch.tensor(el) for el in eval_df.values])\n",
    "\n",
    "\n",
    "class LabelEncoder(object):\n",
    "    # The 'LabelEncoder' splits the raw column strings by 'sep' and converts\n",
    "    # individual elements to categorical labels.\n",
    "    def __init__(self, sep=',', is_tensor=False, mapping=None):\n",
    "        self.sep = sep\n",
    "        self.is_tensor = is_tensor\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __call__(self, df):\n",
    "        mapping = self.mapping\n",
    "        if self.is_tensor:\n",
    "            if not mapping:\n",
    "                labels = set(\n",
    "                    label for col in df.values\n",
    "                    for label in col.split(self.sep)\n",
    "                )\n",
    "                mapping = {label: i for i, label in enumerate(labels)}\n",
    "            x = torch.zeros(len(df), len(mapping))\n",
    "            for i, col in enumerate(df.values):\n",
    "                for label in col.split(self.sep):\n",
    "                    x[i, mapping[label]] = 1\n",
    "            return x\n",
    "        if not mapping:\n",
    "            labels = df[df.columns[0]].unique()\n",
    "            mapping = {label: i for i, label in enumerate(labels)}\n",
    "        return df.replace(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pytorch geometric graph\n",
    "\n",
    "def load_node_tensor(df, index_col, encoders=None):\n",
    "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
    "    x = torch.ones(size=(len(df.index), 1))\n",
    "\n",
    "    if encoders is not None:\n",
    "        xs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        x = torch.cat(xs, dim=-1)\n",
    "        x = x.float()\n",
    "    return x, mapping\n",
    "\n",
    "\n",
    "def load_edge_tensor(df, src_index_col, src_mapping,\n",
    "                     dst_index_col, dst_mapping, encoders=None):\n",
    "    src = [src_mapping[index] for index in df[src_index_col]]\n",
    "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
    "    edge_index = torch.tensor([src, dst])\n",
    "    edge_attr = None\n",
    "    if encoders is not None:\n",
    "        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
    "        edge_attr = torch.cat(edge_attrs, dim=-1)\n",
    "\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "\n",
    "def get_filtered_taxon_nodes():\n",
    "    taxon_nodes_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}taxon_nodes.csv',\n",
    "        index_col='appId',\n",
    "        header=0\n",
    "    )\n",
    "    fasta_has_host = pd.read_csv(\n",
    "        f'{DATASET_DIR}fasta_has_host_gb_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    fasta_has_host['targetAppId'] = fasta_has_host['targetAppId'].astype(int)\n",
    "    ## prune taxonomy tree to only include viruses and specific known hosts\n",
    "    taxon_nodes_df = taxon_nodes_df.loc[\n",
    "        (taxon_nodes_df['taxKingdom'] == 'Viruses') | \n",
    "        (taxon_nodes_df['taxId'].isin(fasta_has_host['targetAppId']))\n",
    "    ]\n",
    "    return taxon_nodes_df\n",
    "\n",
    "\n",
    "def get_filtered_pangenome_nodes():\n",
    "    pangenome_nodes_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}pangenome_nodes.csv',\n",
    "        index_col='appId',\n",
    "        header=0,\n",
    "    )\n",
    "    ## prune segmeneted fasta nodes\n",
    "    pangenome_nodes_df = pangenome_nodes_df.loc[\n",
    "        (pangenome_nodes_df['nodeLabels'] != 'Fasta') |\n",
    "        (pangenome_nodes_df['isSegmented'] == False)\n",
    "    ]\n",
    "    return pangenome_nodes_df\n",
    "\n",
    "\n",
    "def create_pyg_graph():\n",
    "    data = HeteroData()\n",
    "    mappings = {}\n",
    "\n",
    "    ## Load taxons node tensors\n",
    "    taxon_nodes_df = get_filtered_taxon_nodes()\n",
    "    target_taxons = taxon_nodes_df.loc[\n",
    "        taxon_nodes_df['rank'] == 'order'\n",
    "    ]\n",
    "    target_taxon_x, target_taxon_mapping = load_node_tensor(\n",
    "        df=target_taxons,\n",
    "        index_col='appId',\n",
    "    )\n",
    "    data['TargetTaxon'].node_id = torch.arange(len(target_taxon_mapping))\n",
    "    # data['TargetTaxon'].x = target_taxon_x\n",
    "    mappings['TargetTaxon'] = target_taxon_mapping\n",
    "\n",
    "    ## Load pangenome node tensors \n",
    "    pangenome_nodes_df = get_filtered_pangenome_nodes()\n",
    "\n",
    "    for node_type in ['Fasta', 'Hit', 'HitRegion', 'HitFamily']:\n",
    "        node_df = pangenome_nodes_df.loc[\n",
    "            pangenome_nodes_df['nodeLabels'] == node_type\n",
    "        ]\n",
    "        x, mapping = load_node_tensor(\n",
    "            df=node_df,\n",
    "            index_col='appId',\n",
    "        )\n",
    "        data[node_type].node_id = torch.arange(len(mapping))\n",
    "        # data[node_type].x = x\n",
    "        mappings[node_type] = mapping\n",
    "\n",
    "    ## Load pangenome edge tensors\n",
    "    pangenome_edges_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}pangenome_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    # remove edges that are not in the pruned pangenome graph\n",
    "    pangenome_edges_df = pangenome_edges_df.loc[\n",
    "        pangenome_edges_df['sourceAppId'].isin(pangenome_nodes_df.index) &\n",
    "        pangenome_edges_df['targetAppId'].isin(pangenome_nodes_df.index)\n",
    "    ]\n",
    "    config = {\n",
    "        'hasMember': {\n",
    "            'src': 'HitFamily',\n",
    "            'dst': 'Hit',\n",
    "        },\n",
    "        'hasHit': {\n",
    "            'src': 'Fasta',\n",
    "            'dst': 'Hit',\n",
    "        },\n",
    "        'hasRegion': {\n",
    "            'src': 'Fasta',\n",
    "            'dst': 'HitRegion',\n",
    "        },\n",
    "        'hasAffiliate': {\n",
    "            'src': 'HitRegion',\n",
    "            'dst': 'Hit',\n",
    "        },\n",
    "        'hasDownstream': {\n",
    "            'src': 'HitRegion',\n",
    "            'dst': 'HitRegion',\n",
    "        },\n",
    "    }\n",
    "    for rel_type, rel_config in config.items():\n",
    "        edge_df = pangenome_edges_df.loc[\n",
    "            pangenome_edges_df['relationshipType'] == rel_type\n",
    "        ]\n",
    "        edge_index, edge_label = load_edge_tensor(\n",
    "            df=edge_df,\n",
    "            src_index_col='sourceAppId',\n",
    "            src_mapping=mappings[rel_config['src']],\n",
    "            dst_index_col='targetAppId',\n",
    "            dst_mapping=mappings[rel_config['dst']],\n",
    "            encoders={\n",
    "                'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "            },\n",
    "        )\n",
    "        data[rel_config['src'], rel_type, rel_config['dst']].edge_index = edge_index\n",
    "        data[rel_config['src'], rel_type, rel_config['dst']].edge_label = edge_label\n",
    "\n",
    "\n",
    "    ## Load pangenome taxon edge tensors\n",
    "    pangenome_taxon_edges_df = pd.read_csv(\n",
    "        f'{DATASET_DIR}fasta_has_tax_order_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    ## remove edges that are not in the pruned pangenome graph or taxon tree\n",
    "    pangenome_taxon_edges_df = pangenome_taxon_edges_df.loc[\n",
    "        pangenome_taxon_edges_df['sourceAppId'].isin(pangenome_nodes_df.index) &\n",
    "        pangenome_taxon_edges_df['targetAppId'].isin(taxon_nodes_df.index)\n",
    "    ]\n",
    "    \n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=pangenome_taxon_edges_df,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['Fasta'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['TargetTaxon'],\n",
    "    )\n",
    "    data['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_index = edge_index\n",
    "    data['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label = edge_label\n",
    "    return data, mappings\n",
    "\n",
    "\n",
    "def add_context_graph(data, mappings):\n",
    "    taxon_nodes_df = get_filtered_taxon_nodes()\n",
    "    context_taxons = taxon_nodes_df.loc[\n",
    "        taxon_nodes_df['rank'] != 'order'\n",
    "    ]\n",
    "    context_taxon_x, context_taxon_mapping = load_node_tensor(\n",
    "        df=context_taxons,\n",
    "        index_col='appId',\n",
    "    )\n",
    "    data['ContextTaxon'].node_id = torch.arange(len(context_taxon_mapping)) \n",
    "    # data['ContextTaxon'].x = context_taxon_x\n",
    "    mappings['ContextTaxon'] = context_taxon_mapping\n",
    "\n",
    "\n",
    "    ## Load taxon edge tensors\n",
    "    taxon_edges = pd.read_csv(\n",
    "        f'{DATASET_DIR}taxon_has_parent_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    # remove edges that are not in the pruned taxon tree\n",
    "    taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(taxon_nodes_df.index) &\n",
    "        taxon_edges['targetAppId'].isin(taxon_nodes_df.index)\n",
    "    ]\n",
    "    context_taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(mappings['ContextTaxon'].keys()) &\n",
    "        taxon_edges['targetAppId'].isin(mappings['ContextTaxon'].keys())\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=context_taxon_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['ContextTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['ContextTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['ContextTaxon', 'hasParent', 'ContextTaxon'].edge_index = edge_index\n",
    "    data['ContextTaxon', 'hasParent', 'ContextTaxon'].edge_label = edge_label\n",
    "\n",
    "    c_to_t_taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(mappings['ContextTaxon'].keys()) &\n",
    "        taxon_edges['targetAppId'].isin(mappings['TargetTaxon'].keys())\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=c_to_t_taxon_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['ContextTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['TargetTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['ContextTaxon', 'hasParent', 'TargetTaxon'].edge_index = edge_index\n",
    "    data['ContextTaxon', 'hasParent', 'TargetTaxon'].edge_label = edge_label\n",
    "\n",
    "    t_to_c_taxon_edges = taxon_edges.loc[\n",
    "        taxon_edges['sourceAppId'].isin(mappings['TargetTaxon'].keys()) &\n",
    "        taxon_edges['targetAppId'].isin(mappings['ContextTaxon'].keys())\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=t_to_c_taxon_edges,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['TargetTaxon'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['ContextTaxon'],\n",
    "        encoders={\n",
    "            'weight': IdentityEncoder(dtype=torch.float, is_tensor=True)\n",
    "        },\n",
    "    )\n",
    "    data['TargetTaxon', 'hasParent', 'ContextTaxon'].edge_index = edge_index\n",
    "    data['TargetTaxon', 'hasParent', 'ContextTaxon'].edge_label = edge_label\n",
    "\n",
    "    taxon_nodes_df = get_filtered_taxon_nodes()\n",
    "    pangenome_nodes_df = get_filtered_pangenome_nodes()\n",
    "    fasta_has_host = pd.read_csv(\n",
    "        f'{DATASET_DIR}fasta_has_host_gb_edges.csv',\n",
    "        header=0\n",
    "    )\n",
    "    fasta_has_host['targetAppId'] = fasta_has_host['targetAppId'].astype(int)\n",
    "\n",
    "    ## remove edges that are not in the pruned pangenome graph or taxon tree\n",
    "    fasta_has_host = fasta_has_host.loc[\n",
    "        fasta_has_host['sourceAppId'].isin(pangenome_nodes_df.index) &\n",
    "        fasta_has_host['targetAppId'].isin(taxon_nodes_df.index)\n",
    "    ]\n",
    "    edge_index, edge_label = load_edge_tensor(\n",
    "        df=fasta_has_host,\n",
    "        src_index_col='sourceAppId',\n",
    "        src_mapping=mappings['Fasta'],\n",
    "        dst_index_col='targetAppId',\n",
    "        dst_mapping=mappings['ContextTaxon'],\n",
    "    )\n",
    "    data['Fasta', 'hasHostGenbank', 'ContextTaxon'].edge_index = edge_index\n",
    "    data['Fasta', 'hasHostGenbank', 'ContextTaxon'].edge_label = edge_label\n",
    "\n",
    "    return data\n",
    "\n",
    "data, mappings = create_pyg_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data, init data loaders, add additional context and features\n",
    "\n",
    "def add_reverse_edges(data):\n",
    "    data = T.ToUndirected(merge=False)(data)\n",
    "    # Remove \"reverse\" label (redundant when using link loader)\n",
    "    del data['TargetTaxon', 'rev_hasTaxOrder', 'Fasta'].edge_label\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_esm_fts(data, mappings):\n",
    "    with open(DATASET_DIR + 'eb.pkl', 'rb') as pickle_file:\n",
    "        embs = pickle.load(pickle_file)\n",
    "    xs = torch.zeros(data['Hit'].num_nodes, 320)\n",
    "    for emb_idx, emb_name in enumerate(embs['name']):\n",
    "        node_id = mappings['Hit'][emb_name]\n",
    "        node_idx = (data['Hit'].node_id == node_id).nonzero()[0]\n",
    "        xs[node_idx] = torch.tensor(embs['embedding'][emb_idx])\n",
    "    data['Hit'].x = xs\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_degree_fts(data):\n",
    "    node_types, edge_types = data.metadata()\n",
    "    for node_type in node_types:\n",
    "        # if node_type == 'Hit':\n",
    "        #     continue\n",
    "        for edge_type in edge_types:\n",
    "            if node_type not in edge_type:\n",
    "                continue\n",
    "            if 'rev' in edge_type[1]:\n",
    "                continue\n",
    "            if edge_type[1] == 'hasTaxOrder':\n",
    "                continue\n",
    "\n",
    "            idx = 0 if edge_type[0] == node_type else 1\n",
    "            x_degree = degree(\n",
    "                data[edge_type].edge_index[idx],\n",
    "                num_nodes=data[node_type].num_nodes,\n",
    "                dtype=torch.float,\n",
    "            )\n",
    "            if 'x' not in data[node_type]:\n",
    "                data[node_type].x = x_degree.view(-1, 1)\n",
    "            else:\n",
    "                data[node_type].x = torch.cat([data[node_type].x, x_degree.view(-1, 1)], dim=-1)\n",
    "        \n",
    "        data[node_type].x = torch.sum(data[node_type].x, dim=-1, keepdim=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "    \n",
    "def add_metapaths_to_data(data, train_metapaths=None):\n",
    "    metapaths = []\n",
    "    \n",
    "    # HitFamily <-> HitFamily metapath\n",
    "    # TODO: this edge can be added to the original pangenome schema to simplify message-passing\n",
    "    if ('HitFamily', 'metapath_0', 'HitFamily') not in data.edge_types:\n",
    "        metapath = [\n",
    "            ('HitFamily', 'hasMember', 'Hit'),\n",
    "            ('Hit', 'rev_hasAffiliate', 'HitRegion'),\n",
    "            ('HitRegion', 'hasDownstream', 'HitRegion'),\n",
    "            ('HitRegion', 'hasAffiliate', 'Hit'),\n",
    "            ('Hit', 'rev_hasMember', 'HitFamily'),\n",
    "        ]\n",
    "        data = T.AddMetaPaths(metapaths=[metapath], weighted=True)(data)\n",
    "\n",
    "    # Test and val datasets have access to metapaths from training set\n",
    "    if train_metapaths:\n",
    "        data['HitFamily', 'metapath_0', 'HitFamily'].edge_index = torch.cat([\n",
    "            data['HitFamily', 'metapath_0', 'HitFamily'].edge_index,\n",
    "            train_metapaths['HitFamily', 'metapath_0', 'HitFamily'].edge_index\n",
    "        ], dim=-1)\n",
    "        \n",
    "        data['Fasta', 'metapath_0', 'Fasta'].edge_index = train_metapaths['Fasta', 'metapath_0', 'Fasta'].edge_index\n",
    "        return data\n",
    "\n",
    "    # Training set has metapaths that are not available in test and val sets\n",
    "    if ('Fasta', 'metapath_0', 'Fasta') not in data.edge_types:\n",
    "        metapaths.append([\n",
    "            ('Fasta', 'hasTaxOrder', 'TargetTaxon'),\n",
    "            ('TargetTaxon', 'rev_hasTaxOrder', 'Fasta'),\n",
    "        ])\n",
    "\n",
    "    if ('Fasta', 'metapath_1', 'Fasta') not in data.edge_types:\n",
    "        metapaths.append([\n",
    "            ('Fasta', 'hasHostGenbank', 'ContextTaxon'),\n",
    "            ('ContextTaxon', 'rev_hasHostGenbank', 'Fasta'),\n",
    "        ])\n",
    "\n",
    "    # TODO Apply normalization and filtering to reduce number of metapaths:\n",
    "    # https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/bipartite_sage.py\n",
    "\n",
    "    data = T.AddMetaPaths(metapaths=metapaths, weighted=True)(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def enrich_split_data(data, mappings, metapaths=None, add_metapaths=ADD_METAPATHS):\n",
    "    cloned = data.clone()\n",
    "    cloned = add_context_graph(cloned, mappings)\n",
    "    cloned = add_reverse_edges(cloned)\n",
    "    # cloned = add_esm_fts(cloned, mappings)\n",
    "    cloned = add_degree_fts(cloned)\n",
    "    if add_metapaths:\n",
    "        cloned = add_metapaths_to_data(cloned, metapaths)\n",
    "    return cloned\n",
    "\n",
    "\n",
    "def split_data(data):\n",
    "    num_test = TEST_FRACTION\n",
    "    num_val = VAL_FRACTION\n",
    "    print(f'num_val: {num_val}, num_test: {num_test}, num_train: {TRAIN_FRACTION}')\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        num_val=num_val,\n",
    "        num_test=num_test,\n",
    "        # is_undirected=True,\n",
    "        # Of training edges, use 70% for message passing (edge_index)\n",
    "        # and 30% for supervision (edge_label_index)\n",
    "        # disjoint_train_ratio=0.3,\n",
    "        # Generate fixed negative edges in validation and test data\n",
    "        neg_sampling_ratio=NEGATIVE_SAMPLING_RATIO,\n",
    "        # Don't generate negative edges in training set, will be generated by LinkNeighborLoader.\n",
    "        add_negative_train_samples=False,\n",
    "        edge_types=('Fasta', 'hasTaxOrder', 'TargetTaxon'),\n",
    "        rev_edge_types=('TargetTaxon', 'rev_hasTaxOrder', 'Fasta'),\n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    ref_data = data.clone()\n",
    "    return train_data, val_data, test_data, ref_data\n",
    "\n",
    "\n",
    "def get_train_loader(split_data, batch_size=BATCH_SIZE):\n",
    "    # Define train seed edges:\n",
    "    edge_label_index = split_data[(\n",
    "        'Fasta', 'hasTaxOrder', 'TargetTaxon')].edge_label_index\n",
    "    edge_label = split_data[('Fasta', 'hasTaxOrder', 'TargetTaxon')].edge_label\n",
    "\n",
    "    train_loader = LinkNeighborLoader(\n",
    "        data=split_data,\n",
    "        num_neighbors=NUM_NEIGHBORS, #[100, 50],\n",
    "        neg_sampling=dict(mode='binary', amount=NEGATIVE_SAMPLING_RATIO),\n",
    "        edge_label=edge_label,\n",
    "        edge_label_index=(('Fasta', 'hasTaxOrder', 'TargetTaxon'),\n",
    "                          edge_label_index),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # drop_last=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def get_val_loader(split_data, batch_size=BATCH_SIZE):\n",
    "    # Define the validation seed edges:\n",
    "    edge_label_index = split_data[('Fasta', 'hasTaxOrder', 'TargetTaxon')].edge_label_index\n",
    "    edge_label = split_data[('Fasta', 'hasTaxOrder', 'TargetTaxon')].edge_label\n",
    "\n",
    "    val_loader = LinkNeighborLoader(\n",
    "        data=split_data,\n",
    "        num_neighbors=NUM_NEIGHBORS,\n",
    "        # num_neighbors=[10, 10],\n",
    "        edge_label_index=(('Fasta', 'hasTaxOrder', 'TargetTaxon'),\n",
    "                          edge_label_index),\n",
    "        edge_label=edge_label,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        # drop_last=True,\n",
    "    )\n",
    "    return val_loader\n",
    "\n",
    "\n",
    "train_data, val_data, test_data, ref_data = split_data(data)\n",
    "train_data = enrich_split_data(train_data, mappings)\n",
    "val_data = enrich_split_data(val_data, mappings, train_data)\n",
    "test_data = enrich_split_data(test_data, mappings, train_data)\n",
    "ref_data = enrich_split_data(ref_data, mappings, add_metapaths=False)\n",
    "\n",
    "train_loader = get_train_loader(train_data)\n",
    "val_loader = get_val_loader(val_data)\n",
    "test_loader = get_val_loader(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model (v1)\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/main/examples/hetero/recommender_system.py#L145\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(SAGEConv((-1, -1), hidden_channels))\n",
    "        self.batch_norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(BatchNorm(hidden_channels))\n",
    "\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv, batch_norm in zip(self.convs[:-1], self.batch_norms):\n",
    "            x = conv(x, edge_index)\n",
    "            # x = batch_norm(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.convs[-1](x, edge_index)\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        x_src = z_dict['Fasta'][edge_label_index[0]]\n",
    "        x_dst = z_dict['TargetTaxon'][edge_label_index[1]]\n",
    "        return (x_src * x_dst).sum(dim=-1)\n",
    "\n",
    "\n",
    "class ModelV1(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, data):\n",
    "        super().__init__()\n",
    "        self.encoder = GNNEncoder(hidden_channels, out_channels, num_layers)\n",
    "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='sum')\n",
    "        self.decoder = EdgeDecoder()\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index, embeddings=False):\n",
    "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
    "        if embeddings:\n",
    "            return z_dict\n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "\n",
    "def get_model_v1(data):\n",
    "    model = ModelV1(\n",
    "        hidden_channels=128,\n",
    "        out_channels=128,\n",
    "        num_layers=3,\n",
    "        data=data,\n",
    "    ).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model (v2)\n",
    "\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/bipartite_sage.py\n",
    "# https://github.com/pyg-team/pytorch_geometric/blob/36bc9255901fa4afaf03d55fec913ec56888ed53/examples/hetero/bipartite_sage_unsup.py#L202\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FastaEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv3 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "\n",
    "        taxon_x = self.conv1(\n",
    "            x_dict['TargetTaxon'],\n",
    "            edge_index_dict[('TargetTaxon', 'metapath_1', 'TargetTaxon')],\n",
    "        ).relu()\n",
    "\n",
    "        fasta_x = self.conv3(\n",
    "            x_dict['Fasta'],\n",
    "            edge_index_dict[('Fasta', 'metapath_0', 'Fasta')],\n",
    "        ).relu()\n",
    "\n",
    "        fasta_x = self.conv2(\n",
    "            (taxon_x, fasta_x),\n",
    "            edge_index_dict[('TargetTaxon', 'rev_hasTaxOrder', 'Fasta')],\n",
    "        ).relu()\n",
    "\n",
    "        return self.lin(fasta_x)\n",
    "\n",
    "\n",
    "class TaxonEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv3 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        context_x = self.conv1(\n",
    "            x_dict['ContextTaxon'],\n",
    "            edge_index_dict[('ContextTaxon', 'hasParent', 'ContextTaxon')],\n",
    "        ).relu()\n",
    "\n",
    "        target_x = self.conv2(\n",
    "            (x_dict['ContextTaxon'], x_dict['TargetTaxon']),\n",
    "            edge_index_dict[('ContextTaxon', 'rev_hasParent', 'TargetTaxon')],\n",
    "        ).relu()\n",
    "\n",
    "        target_x = self.conv3(\n",
    "            (context_x, target_x),\n",
    "            edge_index_dict[('ContextTaxon', 'rev_hasParent', 'TargetTaxon')],\n",
    "        ).relu()\n",
    "\n",
    "        return self.lin(target_x)\n",
    "\n",
    "\n",
    "\n",
    "class EdgeDecoder(torch.nn.Module):\n",
    "    def forward(self, z_dict, edge_label_index):\n",
    "        x_src = z_dict['Fasta'][edge_label_index[0]]\n",
    "        x_dst = z_dict['TargetTaxon'][edge_label_index[1]]\n",
    "        return (x_src * x_dst).sum(dim=-1)\n",
    "\n",
    "# class EdgeDecoder(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channels):\n",
    "#         super().__init__()\n",
    "#         self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
    "#         self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
    "\n",
    "#     def forward(self, z_dict, edge_label_index):\n",
    "#         row, col = edge_label_index\n",
    "#         z = torch.cat([z_dict['Fasta'][row], z_dict['TargetTaxon'][col]], dim=-1)\n",
    "\n",
    "#         z = self.lin1(z).relu()\n",
    "#         z = self.lin2(z)\n",
    "#         return z.view(-1)\n",
    "\n",
    "\n",
    "class ModelV2(torch.nn.Module):\n",
    "    def __init__(self, num_fastas, num_taxons, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fasta_emb = torch.nn.Embedding(\n",
    "            num_fastas, hidden_channels, device=device)\n",
    "        self.taxon_emb = torch.nn.Embedding(\n",
    "            num_taxons, hidden_channels, device=device)\n",
    "\n",
    "        self.fasta_encoder = FastaEncoder(hidden_channels)\n",
    "        self.taxon_encoder = TaxonEncoder(hidden_channels)\n",
    "        # self.decoder = EdgeDecoder(hidden_channels)\n",
    "        self.decoder = EdgeDecoder()\n",
    "\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index, embeddings=False):\n",
    "        z_dict = {}\n",
    "        z_dict['Fasta'] = self.fasta_emb(x_dict['Fasta'].long())\n",
    "        z_dict['TargetTaxon'] = self.taxon_emb(x_dict['TargetTaxon'].long())\n",
    "\n",
    "        z_dict['Fasta'] = self.fasta_encoder(\n",
    "            x_dict, \n",
    "            edge_index_dict,\n",
    "        )\n",
    "        z_dict['TargetTaxon'] = self.taxon_encoder(\n",
    "            x_dict,\n",
    "            edge_index_dict,\n",
    "        )\n",
    "\n",
    "        if embeddings:\n",
    "            return z_dict\n",
    "            \n",
    "        return self.decoder(z_dict, edge_label_index)\n",
    "\n",
    "\n",
    "def get_model_v2(data):\n",
    "    model = ModelV2(\n",
    "        num_fastas=data['Fasta'].num_nodes,\n",
    "        num_taxons=data['TargetTaxon'].num_nodes,\n",
    "        hidden_channels=64,\n",
    "    ).to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Hierarchical Loss Network\n",
    "'''\n",
    "\n",
    "# modify the dictionary variable in level_dict.py\n",
    "# modify load_dataset.py file to load your dataset as well\n",
    "# if your dataset contains more than 2 levels of hierarchy, be sure to add the classifier layers in models/resnet50.py\n",
    "\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from helper import read_meta\n",
    "\n",
    "\n",
    "class HierarchicalLossNetwork:\n",
    "    '''Logics to calculate the loss of the model.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, metafile_path, hierarchical_labels, device='cpu', total_level=2, alpha=1, beta=0.8, p_loss=3):\n",
    "        '''Param init.\n",
    "        '''\n",
    "        self.total_level = total_level\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.p_loss = p_loss\n",
    "        self.device = device\n",
    "        self.level_one_labels, self.level_two_labels = read_meta(metafile=metafile_path)\n",
    "        self.hierarchical_labels = hierarchical_labels\n",
    "        self.numeric_hierarchy = self.words_to_indices()\n",
    "\n",
    "\n",
    "    def words_to_indices(self):\n",
    "        '''Convert the classes from words to indices.\n",
    "        '''\n",
    "        numeric_hierarchy = {}\n",
    "        for k, v in self.hierarchical_labels.items():\n",
    "            numeric_hierarchy[self.level_one_labels.index(k)] = [self.level_two_labels.index(i) for i in v]\n",
    "\n",
    "        return numeric_hierarchy\n",
    "\n",
    "\n",
    "    def check_hierarchy(self, current_level, previous_level):\n",
    "        '''Check if the predicted class at level l is a children of the class predicted at level l-1 for the entire batch.\n",
    "        '''\n",
    "\n",
    "        #check using the dictionary whether the current level's prediction belongs to the superclass (prediction from the prev layer)\n",
    "        bool_tensor = [not current_level[i] in self.numeric_hierarchy[previous_level[i].item()] for i in range(previous_level.size()[0])]\n",
    "\n",
    "        return torch.FloatTensor(bool_tensor).to(self.device)\n",
    "\n",
    "\n",
    "    def calculate_lloss(self, predictions, true_labels):\n",
    "        '''Calculates the layer loss.\n",
    "        '''\n",
    "\n",
    "        lloss = 0\n",
    "        for l in range(self.total_level):\n",
    "\n",
    "            lloss += nn.CrossEntropyLoss()(predictions[l], true_labels[l])\n",
    "\n",
    "        return self.alpha * lloss\n",
    "\n",
    "    def calculate_dloss(self, predictions, true_labels):\n",
    "        '''Calculate the dependence loss.\n",
    "        '''\n",
    "\n",
    "        dloss = 0\n",
    "        for l in range(1, self.total_level):\n",
    "            current_lvl_pred = torch.argmax(nn.Softmax(dim=1)(predictions[l]), dim=1)\n",
    "            prev_lvl_pred = torch.argmax(nn.Softmax(dim=1)(predictions[l-1]), dim=1)\n",
    "\n",
    "            D_l = self.check_hierarchy(current_lvl_pred, prev_lvl_pred)\n",
    "\n",
    "            l_prev = torch.where(prev_lvl_pred == true_labels[l-1], torch.FloatTensor([0]).to(self.device), torch.FloatTensor([1]).to(self.device))\n",
    "            l_curr = torch.where(current_lvl_pred == true_labels[l], torch.FloatTensor([0]).to(self.device), torch.FloatTensor([1]).to(self.device))\n",
    "\n",
    "            dloss += torch.sum(torch.pow(self.p_loss, D_l*l_prev)*torch.pow(self.p_loss, D_l*l_curr) - 1)\n",
    "\n",
    "        return self.beta * dloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-eval loop\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = total_examples = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(\n",
    "            batch.x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index\n",
    "        )\n",
    "        target = batch['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        # loss = F.mse_loss(pred, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss) * pred.numel()\n",
    "        total_examples += pred.numel()\n",
    "    return total_loss / total_examples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        pred = model(\n",
    "            batch.x_dict,\n",
    "            batch.edge_index_dict,\n",
    "            batch['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index\n",
    "        )\n",
    "        pred = pred.sigmoid().view(-1).long().cpu()\n",
    "\n",
    "        target = batch['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label.long().cpu()\n",
    "        \n",
    "        preds.append(pred)\n",
    "        targets.append(target)\n",
    "\n",
    "\n",
    "    pred = torch.cat(preds, dim=0).numpy()\n",
    "    target = torch.cat(targets, dim=0).numpy()\n",
    "\n",
    "    accuracy = accuracy_score(target, pred)\n",
    "    auc_pr = average_precision_score(target, pred)\n",
    "    # auc_roc = roc_auc_score(target, pred)\n",
    "\n",
    "    return accuracy, auc_pr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_KNN(model, loader, device, k=3):\n",
    "    model.eval()\n",
    "\n",
    "    ref_data.to(device)\n",
    "    dst_emb = model.encoder(ref_data.x_dict, ref_data.edge_index_dict)['TargetTaxon']\n",
    "\n",
    "    # Instantiate k-NN index based on maximum inner product search (MIPS):\n",
    "    mips = MIPSKNNIndex(dst_emb)\n",
    "\n",
    "    # Initialize metrics:\n",
    "    map_metric = LinkPredMAP(k=k).to(device)\n",
    "    precision_metric = LinkPredPrecision(k=k).to(device)\n",
    "    recall_metric = LinkPredRecall(k=k).to(device)\n",
    "\n",
    "    num_processed = 0\n",
    "    for batch in loader:  \n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Compute user embeddings:\n",
    "        emb = model.encoder(batch.x_dict, batch.edge_index_dict)['Fasta']\n",
    "        num_processed += emb.size(0)\n",
    "\n",
    "        # Perform MIPS search:\n",
    "        _, pred_index_mat = mips.search(emb, k)\n",
    "\n",
    "        # Update retrieval metrics:\n",
    "        map_metric.update(pred_index_mat, batch['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index)\n",
    "        precision_metric.update(pred_index_mat, batch['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index)\n",
    "        recall_metric.update(pred_index_mat, batch['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index)\n",
    "\n",
    "    return (\n",
    "        float(map_metric.compute()),\n",
    "        float(precision_metric.compute()),\n",
    "        float(recall_metric.compute()),\n",
    "    )\n",
    "\n",
    "\n",
    "def update_stats(training_stats, epoch_stats):\n",
    "    if training_stats is None:\n",
    "        training_stats = {}\n",
    "        for key in epoch_stats.keys():\n",
    "            training_stats[key] = []\n",
    "    for key, val in epoch_stats.items():\n",
    "        training_stats[key].append(val)\n",
    "    return training_stats\n",
    "\n",
    "\n",
    "def train_and_eval_loop(model, train_loader, val_loader, test_loader):\n",
    "    early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    training_stats = None\n",
    "\n",
    "    for epoch in range(1, 50): \n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        test_acc, test_auc_pr = test(model, test_loader, device)\n",
    "        train_acc, train_auc_pr = test(model, train_loader, device)\n",
    "        if VAL_FRACTION > 0:\n",
    "            val_acc, val_auc_pr = test(model, val_loader, device)\n",
    "\n",
    "        test_lp_map, test_lp_precision, test_lp_recall = test_KNN(model, test_loader, device)\n",
    "        epoch_stats = {\n",
    "            'test_acc': test_acc,\n",
    "            'test_auc_pr': test_auc_pr,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'train_auc_pr': train_auc_pr,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if VAL_FRACTION > 0:\n",
    "            epoch_stats['val_acc'] = val_acc\n",
    "            epoch_stats['val_auc_pr'] = val_auc_pr\n",
    "\n",
    "        training_stats = update_stats(training_stats, epoch_stats)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}\")\n",
    "            print(f\"Train loss: {train_loss:.4f}\")\n",
    "            print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "            print(f\"Train AUC-PR: {train_auc_pr:.4f}\")\n",
    "            print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "            print(f\"Test AUC-PR: {test_auc_pr:.4f}\")\n",
    "            if VAL_FRACTION > 0:\n",
    "                print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "                print(f\"Validation AUC-PR: {val_auc_pr:.4f}\")\n",
    "            print(\n",
    "                f\"Test LP MAP: {test_lp_map:.4f}, \"\n",
    "                f\"Test LP Precision: {test_lp_precision:.4f}, \"\n",
    "                f\"Test LP Recall: {test_lp_recall:.4f}\"\n",
    "            )\n",
    "            print()\n",
    "\n",
    "\n",
    "        if epoch > MIN_EPOCHS \\\n",
    "                and early_stopper.early_stop(test_acc):\n",
    "            break\n",
    "    return training_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_v1(ref_data)\n",
    "stats = train_and_eval_loop(model, train_loader, val_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot stats\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df.set_index('epoch', inplace=True)\n",
    "# stats_df = stats_df[:58]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(3)\n",
    "\n",
    "axs[0].plot(stats_df['train_acc'], label='train_acc')\n",
    "axs[0].plot(stats_df['test_acc'], label='test_acc')\n",
    "if VAL_FRACTION > 0:\n",
    "    axs[0].plot(stats_df['val_acc'], label='val_acc')\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].set_ylim(0, 1)\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "axs[1].plot(stats_df['train_auc_pr'], label='train_auc_pr')\n",
    "axs[1].plot(stats_df['test_auc_pr'], label='test_auc_pr')\n",
    "if VAL_FRACTION > 0:\n",
    "    axs[1].plot(stats_df['val_auc_pr'], label='val_auc_pr')\n",
    "axs[1].set_ylim(0, 1)\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_title('AUC-PR')\n",
    "\n",
    "axs[2].plot(stats_df['train_loss'], label='train_loss')\n",
    "axs[2].set_title('Train loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_top_k_lp_predictions(\n",
    "    model,\n",
    "    data,\n",
    "    mappings,\n",
    "    k,\n",
    "):\n",
    "    data.to(device)\n",
    "    dst_emb = model.encoder(data.x_dict, data.edge_index_dict)['TargetTaxon']\n",
    "\n",
    "    # Instantiate k-NN index based on maximum inner product search (MIPS):\n",
    "    mips = MIPSKNNIndex(dst_emb)\n",
    "\n",
    "    # Initialize metrics:\n",
    "    map_metric = LinkPredMAP(k=k).to(device)\n",
    "    precision_metric = LinkPredPrecision(k=k).to(device)\n",
    "    recall_metric = LinkPredRecall(k=k).to(device)\n",
    "\n",
    "    num_processed = 0\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Compute Fasta embeddings:\n",
    "    emb = model.encoder(data.x_dict, data.edge_index_dict)['Fasta']\n",
    "    num_processed += emb.size(0)\n",
    "\n",
    "    # Perform MIPS search:\n",
    "    _, pred_index_mat = mips.search(emb, k)\n",
    "\n",
    "    print(pred_index_mat)\n",
    "    edge_label_index = data['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_index\n",
    "    # Update retrieval metrics:\n",
    "    map_metric.update(pred_index_mat, edge_label_index)\n",
    "    precision_metric.update(pred_index_mat, edge_label_index)\n",
    "    recall_metric.update(pred_index_mat, edge_label_index)\n",
    "\n",
    "    return (\n",
    "        float(map_metric.compute()),\n",
    "        float(precision_metric.compute()),\n",
    "        float(recall_metric.compute()),\n",
    "    )\n",
    "\n",
    "run_top_k_lp_predictions(model, ref_data, mappings, k=3)\n",
    "# (0.13103356957435608, 0.08196283131837845, 0.24588848650455475)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ref_data['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_index)\n",
    "print(train_data['Fasta', 'hasTaxOrder', 'TargetTaxon'].edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_exhaustive_lp_predictions(\n",
    "    model,\n",
    "    data,\n",
    "    mappings,\n",
    "    threshold=0.5,\n",
    "):\n",
    "    # https://github.com/pyg-team/pytorch_geometric/discussions/6792\n",
    "    file_path = DATASET_DIR + 'lp_pyg_preds.csv'\n",
    "\n",
    "    pangenome_df = pd.read_csv(DATASET_DIR + 'pangenome_nodes.csv')\n",
    "    fasta_df = pangenome_df.loc[\n",
    "        (pangenome_df['nodeLabels'] == 'Fasta') & \n",
    "        (pangenome_df['isSegmented'] == False)\n",
    "    ]\n",
    "\n",
    "    taxon_df = pd.read_csv(DATASET_DIR + 'taxon_nodes.csv')\n",
    "    taxon_df = taxon_df.loc[\n",
    "        (taxon_df['taxKingdom'] == 'Viruses') & \n",
    "        (taxon_df['rank'] == 'order')\n",
    "    ]\n",
    "\n",
    "    num_taxons = len(taxon_df)\n",
    "\n",
    "    # Nested loop over sotus to taxons\n",
    "    outs = []\n",
    "    for index, row in fasta_df[['appId', 'nodeId', 'order']].iterrows():\n",
    "        fasta_i = mappings['Fasta'][row['appId']]\n",
    "        row = torch.tensor([fasta_i] * num_taxons)\n",
    "        col = torch.arange(num_taxons)\n",
    "        edge_label_index = torch.stack([row, col], dim=0)\n",
    "        \n",
    "        pred = model(\n",
    "            data.x_dict,\n",
    "            data.edge_index_dict,\n",
    "            edge_label_index\n",
    "        )\n",
    "        #pred = pred.sigmoid().view(-1).long().cpu()\n",
    "        pred = pred.clamp(min=0, max=1)\n",
    "        pred = (pred>threshold).float()\n",
    "\n",
    "        print(fasta_i)\n",
    "        print(pred)\n",
    "\n",
    "        # ground_truth = torch.zeros(num_taxons)\n",
    "        # ground_truth[mappings['TargetTaxon'][row['order']]] = 1\n",
    "        # print(ground_truth)\n",
    "\n",
    "        # for pred in preds:            \n",
    "        #     if pred > threshold:\n",
    "        #         outs.append({\n",
    "        #             'sourceNodeId': fasta_row['nodeId'],\n",
    "        #             'sourceAppId': fasta_app_id,\n",
    "        #             'targetNodeId': taxon_row['nodeId'],\n",
    "        #             'targetAppId': taxon_row['appId'],\n",
    "        #             'probability': pred,\n",
    "        #         })\n",
    "    # write preds to column\n",
    "    # df = pd.DataFrame(outs)\n",
    "    # df.to_csv(file_path, index=False)\n",
    "    return df\n",
    "\n",
    "preds = run_exhaustive_lp_predictions(\n",
    "    model,\n",
    "    ref_data,\n",
    "    mappings=mappings,\n",
    "    threshold=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv(DATASET_DIR + 'pangenome_nodes.csv')\n",
    "\n",
    "# get unique genome accessions\n",
    "genbank_acc = nodes.loc[\n",
    "    nodes['nodeLabels'] == 'Fasta',\n",
    "    'accession'\n",
    "].unique()\n",
    "\n",
    "print(len(genbank_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "with open(DATASET_DIR + 'gb_cache.pkl', 'rb') as f:\n",
    "    gb_cache = pickle.load(f)\n",
    "\n",
    "\n",
    "def get_host(record):\n",
    "    for feature in record.features:\n",
    "        if feature.type == 'source':\n",
    "            return feature.qualifiers[\"host\"][0]\n",
    "    return None\n",
    "\n",
    "def get_host_tax_id(record):\n",
    "    for feature in record.features:\n",
    "        if feature.type == 'source':\n",
    "            tax_str = feature.qualifiers[\"db_xref\"][0]\n",
    "            return tax_str.split(':')[1]\n",
    "    return None\n",
    "\n",
    "def get_gb_record(accession):\n",
    "    return gb_cache[accession]\n",
    "\n",
    "\n",
    "def get_genbank_host(genbank_acc):\n",
    "    genbank_host_dict = collections.defaultdict(list)\n",
    "    missing = []\n",
    "    for acc in genbank_acc:\n",
    "        try:\n",
    "            record = get_gb_record(acc)\n",
    "            host = get_host_tax_id(record)\n",
    "            genbank_host_dict[host].append(acc)\n",
    "        except:\n",
    "            missing.append(acc)\n",
    "    \n",
    "    # convert genbank_host to df\n",
    "    rows = []\n",
    "    for host, accs in genbank_host_dict.items():\n",
    "        for acc in accs:\n",
    "            rows.append((acc, host))\n",
    "    genbank_host_df = pd.DataFrame(rows, columns=['accession', 'host'])\n",
    "    genbank_host_df.to_csv(DATASET_DIR + 'fasta_has_host_gb_edges.csv', index=False)\n",
    "    print(len(missing))\n",
    "    return genbank_host_df\n",
    "\n",
    "\n",
    "genbank_host = get_genbank_host(genbank_acc)\n",
    "print(len(genbank_host))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = genbank_acc[2]\n",
    "record = get_gb_record(test_acc) \n",
    "print(get_host(record))\n",
    "# print(record)\n",
    "\n",
    "for feature in record.features:\n",
    "    if feature.type == 'source':\n",
    "        tax_str = feature.qualifiers[\"db_xref\"][0]\n",
    "        print(tax_str.split(':')[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    weights_dir = f\"{MODEL_DIR}{timestamp}/\"\n",
    "    if not os.path.exists(weights_dir):\n",
    "        os.makedirs(weights_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), weights_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "\n",
    "pangenome_df = pd.read_csv(DATASET_DIR + 'pangenome_nodes_emb.csv')\n",
    "fasta_df = pangenome_df.loc[\n",
    "    (pangenome_df['nodeLabels'] == 'Fasta') & \n",
    "    (pangenome_df['isSegmented'] == False)\n",
    "]\n",
    "fasta_embs['fastrp'] = fasta_df['fastrp'].apply(\n",
    "            lambda x: [val for val in ast.literal_eval(x)])\n",
    "\n",
    "fasta_embs = pd.DataFrame(fasta_embs['fastrp'].to_list())\n",
    "\n",
    "taxon_df = pd.read_csv(DATASET_DIR + 'taxon_nodes_emb.csv')\n",
    "taxon_df = taxon_df.loc[\n",
    "    (taxon_df['taxKingdom'] == 'Viruses') & \n",
    "    (taxon_df['rank'] == 'order')\n",
    "]\n",
    "taxon_embs['fastrp'] = taxon_df['fastrp'].apply(\n",
    "            lambda x: [val for val in ast.literal_eval(x)])\n",
    "taxon_embs = pd.DataFrame(taxon_embs['fastrp'].to_list())\n",
    "\n",
    "\n",
    "reducer = umap.UMAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_emb = reducer.fit_transform(taxon_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    umap_emb[:, 0],\n",
    "    umap_emb[:, 1],\n",
    "    s=10,\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional topological link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_train_loader(train_data, batch_size=100)\n",
    "\n",
    "for batch in loader:\n",
    "    batch_homogenous = batch.to_homogeneous()\n",
    "    # print(batch_homogenous['edge_index'])\n",
    "    print(batch_homogenous['edge_label'])\n",
    "\n",
    "# print(len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxon_mapping = {v: k for k, v in mappings['TaxonOrder'].items()}\n",
    "print(len(taxon_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_train_loader(train_data, batch_size=2048)\n",
    "\n",
    "fasta_mapping = {v: k for k, v in mappings['Fasta'].items()}\n",
    "taxon_mapping = {v: k for k, v in mappings['TaxonOrder'].items()}\n",
    "fts = []\n",
    "\n",
    "for batch in loader:\n",
    "    batch_homogenous = batch.to_homogeneous()\n",
    "    G = to_networkx(batch_homogenous).to_undirected()\n",
    "\n",
    "    G_connected = G.copy()\n",
    "    isolated_nodes = [node for node in G.nodes() if G.degree(node) == 0]\n",
    "    [G_connected.remove_node(i_n) for i_n in isolated_nodes]\n",
    "\n",
    "    edge_index = batch_homogenous['edge_index'].tolist()\n",
    "    ebunch = list(zip(edge_index[0], edge_index[1]))\n",
    "    lp_iters = [\n",
    "        # ('jaccard_coefficient',\n",
    "        #  lambda: nx.jaccard_coefficient(G, ebunch)),\n",
    "        ('preferential_attachment',\n",
    "            lambda: nx.preferential_attachment(G, ebunch)),\n",
    "        # ('adamic_adar_index',\n",
    "        #  lambda: nx.adamic_adar_index(G_connected, ebunch)),\n",
    "        # ('resource_allocation_index',\n",
    "        #  lambda: nx.resource_allocation_index(G, ebunch)),\n",
    "        # ('common_neighbor_centrality',\n",
    "        #  lambda: nx.common_neighbor_centrality(G, ebunch)),\n",
    "    ]\n",
    "    fts_batch = []\n",
    "\n",
    "    for alg_name, get_iter in lp_iters:\n",
    "        iterable = get_iter()\n",
    "        print(alg_name)\n",
    "        for i, val in enumerate(iterable):\n",
    "            # Handle Adamic Adar division by 0 edge case\n",
    "            if i > len(batch_homogenous['edge_label']) - 1 or torch.isnan(\n",
    "                    batch_homogenous['edge_label'][i]):\n",
    "                print('herex')\n",
    "                if len(fts_batch) > i:\n",
    "                    fts_batch[i][alg_name] = 0\n",
    "                continue\n",
    "\n",
    "            if len(fts_batch) == 0:\n",
    "                print('reset')\n",
    "\n",
    "            if len(fts_batch) != i:\n",
    "                print('here4')\n",
    "                print(len(fts_batch), i)\n",
    "\n",
    "            if len(fts_batch) == i:\n",
    "                fts_batch.append({\n",
    "                    'sourceAppId': fasta_mapping[val[0]],\n",
    "                    'targetAppId': taxon_mapping[val[1]],\n",
    "                    'actual': batch_homogenous[\n",
    "                        'edge_label'][i].long().numpy(),\n",
    "                })\n",
    "            cur_row = fts_batch[i]\n",
    "            cur_row[alg_name] = val[2]\n",
    "    fts.extend(fts_batch)\n",
    "x = pd.DataFrame(fts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_run_link_pred(loader, mappings):\n",
    "    fasta_mapping = {v: k for k, v in mappings['Fasta'].items()}\n",
    "    taxon_mapping = {v: k for k, v in mappings['TaxonOrder'].items()}\n",
    "    fts = []\n",
    "\n",
    "    for batch in loader:\n",
    "        print('here1')\n",
    "        batch_homogenous = batch.to_homogeneous()\n",
    "        G = to_networkx(batch_homogenous).to_undirected()\n",
    "        print('here2')\n",
    "\n",
    "        G_connected = G.copy()\n",
    "        isolated_nodes = [node for node in G.nodes() if G.degree(node) == 0]\n",
    "        [G_connected.remove_node(i_n) for i_n in isolated_nodes]\n",
    "\n",
    "        edge_index = batch_homogenous['edge_index'].tolist()\n",
    "        ebunch = list(zip(edge_index[0], edge_index[1]))\n",
    "        lp_iters = [\n",
    "            # ('jaccard_coefficient',\n",
    "            #  lambda: nx.jaccard_coefficient(G, ebunch)),\n",
    "            ('preferential_attachment',\n",
    "             lambda: nx.preferential_attachment(G, ebunch)),\n",
    "            # ('adamic_adar_index',\n",
    "            #  lambda: nx.adamic_adar_index(G_connected, ebunch)),\n",
    "            # ('resource_allocation_index',\n",
    "            #  lambda: nx.resource_allocation_index(G, ebunch)),\n",
    "            # ('common_neighbor_centrality',\n",
    "            #  lambda: nx.common_neighbor_centrality(G, ebunch)),\n",
    "        ]\n",
    "        print('here3')\n",
    "        fts_batch = []\n",
    "        for alg_name, get_iter in lp_iters:\n",
    "            iterable = get_iter()\n",
    "            \n",
    "            print(alg_name)\n",
    "            for i, val in enumerate(iterable):\n",
    "                # Handle Adamic Adar division by 0 edge case\n",
    "                if i > len(batch_homogenous['edge_label']) - 1 or torch.isnan(\n",
    "                        batch_homogenous['edge_label'][i]):\n",
    "                    if len(fts_batch) > i:\n",
    "                        fts_batch[i][alg_name] = 0\n",
    "                    continue\n",
    "\n",
    "                if len(fts_batch) == 0:\n",
    "                    print('reset')\n",
    "\n",
    "                if len(fts_batch) != i:\n",
    "                    print('here4')\n",
    "                    print(len(fts_batch), i)\n",
    "\n",
    "                if len(fts_batch) == i:\n",
    "                    fts_batch.append({\n",
    "                        'sourceAppId': fasta_mapping[val[0]],\n",
    "                        'targetAppId': taxon_mapping[val[1]],\n",
    "                        'actual': batch_homogenous[\n",
    "                            'edge_label'][i].long().numpy(),\n",
    "                    })\n",
    "                cur_row = fts_batch[i]\n",
    "                cur_row[alg_name] = val[2]\n",
    "        fts.extend(fts_batch)\n",
    "    return pd.DataFrame(fts)\n",
    "\n",
    "\n",
    "def nx_evaluate(ground_truths, preds):\n",
    "    accuracy = accuracy_score(ground_truths, preds)\n",
    "    precision = precision_score(ground_truths, preds)\n",
    "    recall = recall_score(ground_truths, preds)\n",
    "    f1 = f1_score(ground_truths, preds)\n",
    "    roc_auc = roc_auc_score(ground_truths, preds)\n",
    "    average_precision = average_precision_score(ground_truths, preds)\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\",\n",
    "               \"f1\", \"roc_auc\", \"average_precision\"]\n",
    "    values = [accuracy, precision, recall, f1, roc_auc, average_precision]\n",
    "    return pd.DataFrame(data={'metric': metrics, 'value': values})\n",
    "\n",
    "\n",
    "def nx_evaluate_link_pred(fts):\n",
    "    cols = [\n",
    "        # 'jaccard_coefficient',\n",
    "        'preferential_attachment',\n",
    "        # 'adamic_adar_index',\n",
    "        # 'resource_allocation_index',\n",
    "        # 'common_neighbor_centrality',\n",
    "    ]\n",
    "    stats = []\n",
    "    ground_truths = torch.tensor(fts['actual'].values.astype(int))\n",
    "    ground_truths = ground_truths.clamp(min=0, max=1)\n",
    "    for lp_alg in cols:\n",
    "        preds = torch.tensor(fts[lp_alg].values.astype(float))\n",
    "        # TODO: review if mid-point is best threshold for all algos\n",
    "        threshold = torch.div(torch.min(preds) + torch.max(preds), 2)\n",
    "        preds = (preds > threshold).clamp(min=0, max=1).long()\n",
    "        stats.append(\n",
    "            (lp_alg, evaluate(ground_truths, preds))\n",
    "        )\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, mappings = create_pyg_graph()\n",
    "# train_data, val_data, test_data = split_data(data)\n",
    "loader = get_train_loader(train_data)\n",
    "\n",
    "lp_fts = nx_run_link_pred(loader, mappings)\n",
    "stats = nx_evaluate_link_pred(lp_fts)\n",
    "\n",
    "# 1002684\n",
    "# 0\n",
    "# 0 513704"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ljp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
